"""
Weighted macro-averages between tasks. The command to produce each are above the weighted average.

Weights are fit to optimize rank correlation with PC-1 on the --tasks-dev-target
"""

WEIGHTED_AVERAGES = {
    # olmo-cookbook compute-weights \
    #     --tasks-dev-small olmo3:dev:1b:macro:rc \
    #     --tasks-dev-target olmo3:dev:7b:micro \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:1b:macro:rc:w_avg": {
        "arc:rc::full": 0.15636558701895611,
        "coqa:rc::gen2mc": 0.017431157577026464,
        "csqa:rc::olmes:full": 0.0174391870179538,
        "drop:rc::gen2mc": 0.2219235074591158,
        "hellaswag:rc::olmes:full": 0.012576789402743928,
        "jeopardy:rc::gen2mc": 0.012500273914595012,
        "lab_bench_dbqa": 0.031610236151033075,
        "lab_bench_protocolqa": 0.0006264975642724046,
        "medmcqa:rc::none": 0.056887333283218315,
        "mmlu:rc": 0.3373099205061045,
        "naturalqs:rc::gen2mc": 0.001784683758872751,
        "piqa:rc::olmes:full": 0.016064902980663657,
        "socialiqa:rc::olmes:full": 0.058349079688022845,
        "squad:rc::gen2mc": 0.038419696679133725,
        "winogrande:rc::olmes:full": 0.020711146998287473,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-small olmo3:dev:1b:micro:rc \
    #     --tasks-dev-target olmo3:dev:7b:micro \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:1b:micro:rc:w_avg": {
        "arc:rc::full": 0.008797814491317004,
        "coqa:rc::gen2mc": 0.007728275721376411,
        "csqa:rc::olmes:full": 0.02291206005917082,
        "drop:rc::gen2mc": 0.017202885260556144,
        "hellaswag:rc::olmes:full": 0.005994975059766831,
        "jeopardy:rc::gen2mc": 0.01745334205686385,
        "lab_bench_dbqa": 0.017572063144243717,
        "lab_bench_protocolqa": 0.008606833954314386,
        "medmcqa:rc::none": 0.0094006759960794,
        "mmlu_abstract_algebra:rc::olmes": 0.002120981797156597,
        "mmlu_anatomy:rc::olmes": 0.005737649712371782,
        "mmlu_astronomy:rc::olmes": 0.03179406296373593,
        "mmlu_business_ethics:rc::olmes": 0.0022835754529588526,
        "mmlu_clinical_knowledge:rc::olmes": 0.0020708176870735786,
        "mmlu_college_biology:rc::olmes": 0.0008916528061140045,
        "mmlu_college_chemistry:rc::olmes": 9.155455855517962e-05,
        "mmlu_college_computer_science:rc::olmes": 0.008233972790313225,
        "mmlu_college_mathematics:rc::olmes": 0.020122674060070896,
        "mmlu_college_medicine:rc::olmes": 0.0010955025987122566,
        "mmlu_college_physics:rc::olmes": 0.03373916584543533,
        "mmlu_computer_security:rc::olmes": 0.0038734358461089712,
        "mmlu_conceptual_physics:rc::olmes": 0.016732107311277278,
        "mmlu_econometrics:rc::olmes": 0.0018534053728114994,
        "mmlu_electrical_engineering:rc::olmes": 0.004092287677353737,
        "mmlu_elementary_mathematics:rc::olmes": 0.02048175582413836,
        "mmlu_formal_logic:rc::olmes": 0.011957222638293704,
        "mmlu_global_facts:rc::olmes": 0.01000116921275839,
        "mmlu_high_school_biology:rc::olmes": 0.003473373838605454,
        "mmlu_high_school_chemistry:rc::olmes": 0.0010702165473203404,
        "mmlu_high_school_computer_science:rc::olmes": 0.027083074741733115,
        "mmlu_high_school_european_history:rc::olmes": 0.015838848616066218,
        "mmlu_high_school_geography:rc::olmes": 0.014910404531154673,
        "mmlu_high_school_government_and_politics:rc::olmes": 0.008965880432270085,
        "mmlu_high_school_macroeconomics:rc::olmes": 0.019554959861025442,
        "mmlu_high_school_mathematics:rc::olmes": 0.13150747377332675,
        "mmlu_high_school_microeconomics:rc::olmes": 0.021692898238787523,
        "mmlu_high_school_physics:rc::olmes": 0.0031445337771405556,
        "mmlu_high_school_psychology:rc::olmes": 0.0015658458376160648,
        "mmlu_high_school_statistics:rc::olmes": 0.0272684715240589,
        "mmlu_high_school_us_history:rc::olmes": 0.003225593266223907,
        "mmlu_high_school_world_history:rc::olmes": 0.0058826376208329775,
        "mmlu_human_aging:rc::olmes": 0.03700927487906425,
        "mmlu_human_sexuality:rc::olmes": 0.007144733649711107,
        "mmlu_international_law:rc::olmes": 0.002835616652827022,
        "mmlu_jurisprudence:rc::olmes": 0.03672121071300694,
        "mmlu_logical_fallacies:rc::olmes": 0.023797703516173746,
        "mmlu_machine_learning:rc::olmes": 0.0030556560724415882,
        "mmlu_management:rc::olmes": 0.003355518754952685,
        "mmlu_marketing:rc::olmes": 0.005927503915019833,
        "mmlu_medical_genetics:rc::olmes": 0.0065302878342048554,
        "mmlu_miscellaneous:rc::olmes": 0.006633396590845175,
        "mmlu_moral_disputes:rc::olmes": 0.007994825714304205,
        "mmlu_moral_scenarios:rc::olmes": 0.08192680216241961,
        "mmlu_nutrition:rc::olmes": 0.003042891324608145,
        "mmlu_philosophy:rc::olmes": 0.01947203998440123,
        "mmlu_prehistory:rc::olmes": 0.006273327901071644,
        "mmlu_professional_accounting:rc::olmes": 0.01266668226096625,
        "mmlu_professional_law:rc::olmes": 0.01255294195332644,
        "mmlu_professional_medicine:rc::olmes": 0.0020308885703014005,
        "mmlu_professional_psychology:rc::olmes": 0.032221270073652115,
        "mmlu_public_relations:rc::olmes": 0.0020603365138861006,
        "mmlu_security_studies:rc::olmes": 0.004774168092239327,
        "mmlu_sociology:rc::olmes": 0.00886374991572171,
        "mmlu_us_foreign_policy:rc::olmes": 0.00019788941050034845,
        "mmlu_virology:rc::olmes": 0.028552832674717452,
        "mmlu_world_religions:rc::olmes": 0.014381288466263728,
        "naturalqs:rc::gen2mc": 0.001465772299986713,
        "piqa:rc::olmes:full": 0.0006469953240058733,
        "socialiqa:rc::olmes:full": 0.00802694001337345,
        "squad:rc::gen2mc": 0.010554327326975851,
        "winogrande:rc::olmes:full": 0.031256994933940886,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-small olmo3:dev:1b:macro:bpb \
    #     --tasks-dev-target olmo3:dev:7b:micro \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:1b:macro:bpb:w_avg": {
        "arc:bpb::full": 0.007432696579003527,
        "codex_humaneval:3shot:bpb::none": 0.11164372299995005,
        "coqa:bpb::gen2mc": 0.039077957619086806,
        "csqa:bpb::olmes:full": 0.0634738151249363,
        "drop:bpb::gen2mc": 0.1780219434956499,
        "hellaswag:bpb::olmes:full": 0.059885417209609985,
        "jeopardy:bpb::gen2mc": 0.0049741358436264085,
        "lab_bench_dbqa:bpb": 0.018567073398582778,
        "lab_bench_protocolqa:bpb": 0.027555796040518096,
        "lambada:bpb": 0.007517324252832587,
        "mbpp:3shot:bpb::none": 0.11307904208688783,
        "medmcqa:bpb::none": 0.07299316171369184,
        "mmlu:bpb": 0.21278154141147937,
        "naturalqs:bpb::gen2mc": 0.007382548296859934,
        "piqa:bpb::olmes:full": 0.014596715980697887,
        "socialiqa:bpb::olmes:full": 0.03726218689837656,
        "squad:bpb::gen2mc": 0.02045608224672027,
        "winogrande:bpb::olmes:full": 0.0032988388014899258,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-small olmo3:dev:1b:micro:bpb \
    #     --tasks-dev-target olmo3:dev:7b:micro \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:1b:micro:bpb:w_avg": {
        "arc_challenge:bpb::olmes:full": 0.018274087023659322,
        "arc_easy:bpb::olmes:full": 0.004585486551393022,
        "basic_skills_arithmetic:bpb::olmes": 0.1073240340843333,
        "basic_skills_coding:bpb::olmes": 0.002511192035473446,
        "basic_skills_common_knowledge:bpb::olmes": 0.004124898427707438,
        "basic_skills_logical_reasoning:bpb::olmes": 0.004364641927638146,
        "basic_skills_pattern:bpb::olmes": 0.004108560901799638,
        "basic_skills_string_operations:bpb::olmes": 0.006882539428969896,
        "codex_humaneval:3shot:bpb::none": 0.005808339287592693,
        "coqa:bpb::gen2mc": 0.009829522449984737,
        "csqa:bpb::olmes:full": 0.003576861076023748,
        "drop:bpb::gen2mc": 0.010625689132256208,
        "hellaswag:bpb::olmes:full": 0.0009648968348931137,
        "jeopardy:bpb::gen2mc": 0.0016194911748816283,
        "lab_bench_dbqa:bpb": 0.01794504564408441,
        "lab_bench_protocolqa:bpb": 0.01638329366701465,
        "lambada:bpb": 0.005272013208484843,
        "mbpp:3shot:bpb::none": 0.007146093675157426,
        "medmcqa:bpb::none": 0.005210432526189656,
        "minerva_math_algebra:bpb::olmes": 0.002512186642526219,
        "minerva_math_counting_and_probability:bpb::olmes": 0.017516373078626597,
        "minerva_math_geometry:bpb::olmes": 4.9743515347649376e-05,
        "minerva_math_intermediate_algebra:bpb::olmes": 0.0025928043668242556,
        "minerva_math_number_theory:bpb::olmes": 0.004094694481595177,
        "minerva_math_prealgebra:bpb::olmes": 0.002023501724195347,
        "minerva_math_precalculus:bpb::olmes": 0.0170815889134783,
        "mmlu_abstract_algebra:bpb::olmes": 0.008046748596576793,
        "mmlu_anatomy:bpb::olmes": 0.009178255537913979,
        "mmlu_astronomy:bpb::olmes": 0.0066227742594237565,
        "mmlu_business_ethics:bpb::olmes": 0.004232571061243686,
        "mmlu_clinical_knowledge:bpb::olmes": 0.0033713720205329816,
        "mmlu_college_biology:bpb::olmes": 0.0022847236027105384,
        "mmlu_college_chemistry:bpb::olmes": 0.014674015550051019,
        "mmlu_college_computer_science:bpb::olmes": 0.021289663763667695,
        "mmlu_college_mathematics:bpb::olmes": 0.0035058400710138928,
        "mmlu_college_medicine:bpb::olmes": 0.0030974105629151365,
        "mmlu_college_physics:bpb::olmes": 0.009114105106536452,
        "mmlu_computer_security:bpb::olmes": 0.01790619088178488,
        "mmlu_conceptual_physics:bpb::olmes": 0.004519976713176464,
        "mmlu_econometrics:bpb::olmes": 0.000978687874355912,
        "mmlu_electrical_engineering:bpb::olmes": 0.000557685849353467,
        "mmlu_elementary_mathematics:bpb::olmes": 0.027354222303977455,
        "mmlu_formal_logic:bpb::olmes": 0.008135540574957386,
        "mmlu_global_facts:bpb::olmes": 0.001292049669275159,
        "mmlu_high_school_biology:bpb::olmes": 0.02046145805024385,
        "mmlu_high_school_chemistry:bpb::olmes": 0.011962865844193909,
        "mmlu_high_school_computer_science:bpb::olmes": 0.020430990826496433,
        "mmlu_high_school_european_history:bpb::olmes": 0.0018230741553681832,
        "mmlu_high_school_geography:bpb::olmes": 0.017297354389973377,
        "mmlu_high_school_government_and_politics:bpb::olmes": 0.00046550491882111885,
        "mmlu_high_school_macroeconomics:bpb::olmes": 0.0003151723121564308,
        "mmlu_high_school_mathematics:bpb::olmes": 0.013064719856430767,
        "mmlu_high_school_microeconomics:bpb::olmes": 0.005835973725764679,
        "mmlu_high_school_physics:bpb::olmes": 0.0001276371796803783,
        "mmlu_high_school_psychology:bpb::olmes": 0.001282850374791424,
        "mmlu_high_school_statistics:bpb::olmes": 0.002834619358631748,
        "mmlu_high_school_us_history:bpb::olmes": 0.018467976022469898,
        "mmlu_high_school_world_history:bpb::olmes": 0.0230598197638457,
        "mmlu_human_aging:bpb::olmes": 0.014267896931382298,
        "mmlu_human_sexuality:bpb::olmes": 0.0014012894995245908,
        "mmlu_international_law:bpb::olmes": 0.018301505054970506,
        "mmlu_jurisprudence:bpb::olmes": 0.006852818328723955,
        "mmlu_logical_fallacies:bpb::olmes": 0.004346208497810465,
        "mmlu_machine_learning:bpb::olmes": 0.0016162571188823927,
        "mmlu_management:bpb::olmes": 0.002026254354472209,
        "mmlu_marketing:bpb::olmes": 0.0011898541875656472,
        "mmlu_medical_genetics:bpb::olmes": 0.00743097922431955,
        "mmlu_miscellaneous:bpb::olmes": 0.01869230905954769,
        "mmlu_moral_disputes:bpb::olmes": 0.005763354936983655,
        "mmlu_moral_scenarios:bpb::olmes": 0.005651386862409582,
        "mmlu_nutrition:bpb::olmes": 0.004827279982619425,
        "mmlu_philosophy:bpb::olmes": 0.014366164601059285,
        "mmlu_prehistory:bpb::olmes": 0.01229642383266798,
        "mmlu_professional_accounting:bpb::olmes": 0.0010315645749541709,
        "mmlu_professional_law:bpb::olmes": 0.0028135445537147293,
        "mmlu_professional_medicine:bpb::olmes": 0.002090476000648137,
        "mmlu_professional_psychology:bpb::olmes": 0.012089864311680064,
        "mmlu_public_relations:bpb::olmes": 0.0007887475351766321,
        "mmlu_security_studies:bpb::olmes": 0.0015266010656280668,
        "mmlu_sociology:bpb::olmes": 0.014894776756321301,
        "mmlu_us_foreign_policy:bpb::olmes": 0.00031646285168802874,
        "mmlu_virology:bpb::olmes": 0.020847102823284926,
        "mmlu_world_religions:bpb::olmes": 0.009927598986661996,
        "mt_mbpp_v2fix:bash:bpb": 0.01764719712562093,
        "mt_mbpp_v2fix:c:bpb": 0.012974696158839685,
        "mt_mbpp_v2fix:cpp:bpb": 0.00352461874471829,
        "mt_mbpp_v2fix:csharp:bpb": 0.011264312189671633,
        "mt_mbpp_v2fix:go:bpb": 0.01499614120942969,
        "mt_mbpp_v2fix:haskell:bpb": 0.011481887350802671,
        "mt_mbpp_v2fix:java:bpb": 0.006339593337822159,
        "mt_mbpp_v2fix:javascript:bpb": 0.012447092371395432,
        "mt_mbpp_v2fix:matlab:bpb": 0.001071137141091061,
        "mt_mbpp_v2fix:php:bpb": 0.055712419903978655,
        "mt_mbpp_v2fix:python:bpb": 0.005004240008774439,
        "mt_mbpp_v2fix:r:bpb": 0.009491597701045166,
        "mt_mbpp_v2fix:ruby:bpb": 0.008193052478749744,
        "mt_mbpp_v2fix:rust:bpb": 0.0009549471189657821,
        "mt_mbpp_v2fix:scala:bpb": 0.002008104974065133,
        "mt_mbpp_v2fix:swift:bpb": 0.002333571774889047,
        "mt_mbpp_v2fix:typescript:bpb": 0.026488222629132648,
        "naturalqs:bpb::gen2mc": 0.0011019465594176115,
        "piqa:bpb::olmes:full": 0.015126604666699642,
        "socialiqa:bpb::olmes:full": 0.0027698715646968324,
        "squad:bpb::gen2mc": 0.02760320830467563,
        "winogrande:bpb::olmes:full": 8.895219435553339e-05,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-target olmo3:dev:7b:macro \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:7b:macro:w_avg": {
        "arc:mc::full": 0.015473383469915806,
        "basic:rc": 0.026087110095063835,
        "codex_humaneval:3shot::olmo3": 0.050721742788123116,
        "coqa::olmes": 0.028259641369511077,
        "coqa:mc::gen2mc": 0.05256028272636436,
        "cruxeval_input:pass@5": 0.019711756109696724,
        "cruxeval_output:pass@5": 0.004046902863963933,
        "csqa:mc::olmes:full": 0.047825412038840526,
        "drop::olmes": 0.10497794379393241,
        "drop:mc::gen2mc": 0.012941446297807808,
        "gsm-symb": 0.01514602367285424,
        "gsm8k::olmes": 0.06324380242732966,
        "hellaswag:rc::olmes:full": 0.005150769454392643,
        "jeopardy::olmes": 0.03521501857550169,
        "jeopardy:mc::gen2mc": 0.014091749802980651,
        "lambada": 0.0011709447916942108,
        "mbpp:3shot::olmo3": 0.051681846086787,
        "medmcqa:mc::none": 0.0027161719970252773,
        "medqa_en:mc::none": 0.02070134064833692,
        "minerva": 0.11521151449075422,
        "mmlu:mc": 0.10283968296212243,
        "naturalqs::olmes": 0.03572308628168784,
        "naturalqs:mc::gen2mc": 0.0034575846290450882,
        "piqa:mc::olmes:full": 0.0533569775912057,
        "sciq:mc::olmo3": 0.03989676560786533,
        "socialiqa:mc::olmes:full": 0.04654005336850668,
        "squad::olmes": 0.0016648441747766961,
        "squad:mc::gen2mc": 0.02017513251483939,
        "winogrande:rc::olmes:full": 0.009411069369074827,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-target olmo3:dev:7b:micro \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:7b:micro:w_avg": {
        "arc_challenge:mc::olmes:full": 0.016466030885418974,
        "arc_easy:mc::olmes:full": 0.007459640950968453,
        "codex_humaneval:3shot::olmo3": 0.00196973336497905,
        "coqa::olmes": 0.007993426912611781,
        "coqa:mc::gen2mc": 0.0003867164957878487,
        "cruxeval_input:pass@5": 0.017209588732826818,
        "cruxeval_output:pass@5": 0.001066679650677994,
        "csqa:mc::olmes:full": 0.0003979850268278439,
        "drop::olmes": 0.028278490804606777,
        "drop:mc::gen2mc": 0.004320319745622549,
        "gsm8k::olmes": 0.007997649066139678,
        "gsm_symbolic::olmo3": 0.02524892107020496,
        "gsm_symbolic:p1::olmo3": 0.002365241306747053,
        "gsm_symbolic:p2::olmo3": 0.0353533807937371,
        "hellaswag:rc::olmes:full": 0.006617316524146125,
        "jeopardy::olmes": 0.00825698825538478,
        "jeopardy:mc::gen2mc": 0.005802579720448536,
        "lambada": 0.009683872213175234,
        "mbpp:3shot::olmo3": 0.015421811989074741,
        "medmcqa:mc::none": 0.010428756305961925,
        "medqa_en:mc::none": 0.021778138783424196,
        "minerva_math_algebra::olmes": 0.0024647713035461536,
        "minerva_math_counting_and_probability::olmes": 0.02660807704084201,
        "minerva_math_geometry::olmes": 0.006266955839973054,
        "minerva_math_intermediate_algebra::olmes": 0.006584608882601187,
        "minerva_math_number_theory::olmes": 0.0005271738415726158,
        "minerva_math_prealgebra::olmes": 0.0030307343540250716,
        "minerva_math_precalculus::olmes": 0.00018924702820516398,
        "mmlu:mc": 0.013946594852336331,
        "mmlu_abstract_algebra:mc::olmes": 0.01155709139469866,
        "mmlu_anatomy:mc::olmes": 0.0025206935573361174,
        "mmlu_astronomy:mc::olmes": 0.011653190628682156,
        "mmlu_business_ethics:mc::olmes": 0.007868942374955888,
        "mmlu_clinical_knowledge:mc::olmes": 0.021909686766622003,
        "mmlu_college_biology:mc::olmes": 7.930201106861664e-05,
        "mmlu_college_chemistry:mc::olmes": 0.005745878643839664,
        "mmlu_college_computer_science:mc::olmes": 0.002113641337425486,
        "mmlu_college_mathematics:mc::olmes": 0.0027751603297574427,
        "mmlu_college_medicine:mc::olmes": 0.0022566172333647466,
        "mmlu_college_physics:mc::olmes": 0.016188831070128983,
        "mmlu_computer_security:mc::olmes": 0.0025480432193317154,
        "mmlu_conceptual_physics:mc::olmes": 0.05434259819563279,
        "mmlu_econometrics:mc::olmes": 0.006816230553401793,
        "mmlu_electrical_engineering:mc::olmes": 0.04669505402564458,
        "mmlu_elementary_mathematics:mc::olmes": 0.026154163936604757,
        "mmlu_formal_logic:mc::olmes": 0.005733154555432161,
        "mmlu_global_facts:mc::olmes": 0.032067061263674125,
        "mmlu_high_school_biology:mc::olmes": 0.0004402885498605287,
        "mmlu_high_school_chemistry:mc::olmes": 0.003920523260892913,
        "mmlu_high_school_computer_science:mc::olmes": 6.396472303206553e-05,
        "mmlu_high_school_european_history:mc::olmes": 0.0021535623048203594,
        "mmlu_high_school_geography:mc::olmes": 0.0021669610045507584,
        "mmlu_high_school_government_and_politics:mc::olmes": 0.008179786129359375,
        "mmlu_high_school_macroeconomics:mc::olmes": 0.008389723574861924,
        "mmlu_high_school_mathematics:mc::olmes": 0.010148388953258031,
        "mmlu_high_school_microeconomics:mc::olmes": 0.0027658461453579112,
        "mmlu_high_school_physics:mc::olmes": 0.01078613848667904,
        "mmlu_high_school_psychology:mc::olmes": 0.013154979697719393,
        "mmlu_high_school_statistics:mc::olmes": 0.0010860457916099934,
        "mmlu_high_school_us_history:mc::olmes": 0.01255808797668154,
        "mmlu_high_school_world_history:mc::olmes": 0.010118685614294305,
        "mmlu_human_aging:mc::olmes": 0.00037090591988581047,
        "mmlu_human_sexuality:mc::olmes": 0.00923862052236967,
        "mmlu_international_law:mc::olmes": 0.0037765542518853846,
        "mmlu_jurisprudence:mc::olmes": 0.00789398583074267,
        "mmlu_logical_fallacies:mc::olmes": 0.03447396416074928,
        "mmlu_machine_learning:mc::olmes": 0.0066375880951647,
        "mmlu_management:mc::olmes": 0.003491488651447483,
        "mmlu_marketing:mc::olmes": 0.016273627246796263,
        "mmlu_medical_genetics:mc::olmes": 0.01137740782218691,
        "mmlu_miscellaneous:mc::olmes": 0.024886379834953194,
        "mmlu_moral_disputes:mc::olmes": 0.03390894204568508,
        "mmlu_moral_scenarios:mc::olmes": 0.007428238331571243,
        "mmlu_nutrition:mc::olmes": 0.0019326183380770961,
        "mmlu_philosophy:mc::olmes": 0.006085427498987143,
        "mmlu_prehistory:mc::olmes": 0.0008485048669759796,
        "mmlu_professional_accounting:mc::olmes": 0.003834733196190452,
        "mmlu_professional_law:mc::olmes": 0.014233999090516238,
        "mmlu_professional_medicine:mc::olmes": 0.014906862483433942,
        "mmlu_professional_psychology:mc::olmes": 0.04580821744999965,
        "mmlu_public_relations:mc::olmes": 0.013432203485639246,
        "mmlu_security_studies:mc::olmes": 0.008410545487616489,
        "mmlu_sociology:mc::olmes": 0.00903473081232678,
        "mmlu_us_foreign_policy:mc::olmes": 0.02219852426910933,
        "mmlu_virology:mc::olmes": 0.005539599235381527,
        "mmlu_world_religions:mc::olmes": 0.00780738559435324,
        "naturalqs::olmes": 0.011793230980533592,
        "naturalqs:mc::gen2mc": 0.0006095600628126257,
        "piqa:mc::olmes:full": 0.003966169575930312,
        "sciq:mc::olmo3": 0.004220642697566293,
        "socialiqa:mc::olmes:full": 0.006745796561332668,
        "squad::olmes": 0.004406104954963072,
        "squad:mc::gen2mc": 0.0014037231432681922,
        "winogrande:rc::olmes:full": 0.013943962449095128,
    },
}


"""
When for some weight sets, we manually define a [min, max] we want the fitted weights to be within.

For example, in "olmo3:dev:1b:macro:rc" we want "mmlu:bpb" to be at least 20% of the weighted score, and at most 50% of the weighted score.
"""
MANUAL_PRIOR = {
    "olmo3:dev:1b:macro:rc": {
        "arc:rc::full": [0.1, 0.5],
        "mmlu:rc": [0.2, 0.5],
        "drop:rc::gen2mc": [0, 0.1],
        "lab_bench_dbqa": [0, 0.05],
    },
    "olmo3:dev:1b:macro:bpb": {
        "codex_humaneval:3shot:bpb::none": [0.1, 0.5],
        "hellaswag:bpb::olmes:full": [0.05, 0.5],
        "mbpp:3shot:bpb::none": [0.1, 0.5],
        "mmlu:bpb": [0.2, 0.5],
        "arc:rc:full": [0.1, 0.5],
        "drop:bpb::gen2mc": [0, 0.1],
    },
    "olmo3:dev:7b:macro": {
        "codex_humaneval:3shot::olmo3": [0.05, 0.5],
        "gsm8k::olmes": [0.05, 0.5],
        "mbpp:3shot::olmo3": [0.05, 0.5],
        "minerva": [0.1, 0.5],
        "mmlu:mc": [0.1, 0.5],
    },
}
