"""
Weighted macro-averages between tasks. The command to produce each are above the weighted average.

Weights are fit to optimize rank correlation with PC-1 on the --tasks-dev-target
"""

WEIGHTED_AVERAGES = {
    # olmo-cookbook compute-weights \
    #     --tasks-dev-small olmo3:dev:1b:micro:bpb \
    #     --tasks-dev-target olmo3:dev:7b:micro \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:1b:micro:bpb:w_avg": {
        "arc_challenge:bpb::olmes:full": 0.018274087023659322,
        "arc_easy:bpb::olmes:full": 0.004585486551393022,
        "basic_skills_arithmetic:bpb::olmes": 0.1073240340843333,
        "basic_skills_coding:bpb::olmes": 0.002511192035473446,
        "basic_skills_common_knowledge:bpb::olmes": 0.004124898427707438,
        "basic_skills_logical_reasoning:bpb::olmes": 0.004364641927638146,
        "basic_skills_pattern:bpb::olmes": 0.004108560901799638,
        "basic_skills_string_operations:bpb::olmes": 0.006882539428969896,
        "codex_humaneval:3shot:bpb::none": 0.005808339287592693,
        "coqa:bpb::gen2mc": 0.009829522449984737,
        "csqa:bpb::olmes:full": 0.003576861076023748,
        "drop:bpb::gen2mc": 0.010625689132256208,
        "hellaswag:bpb::olmes:full": 0.0009648968348931137,
        "jeopardy:bpb::gen2mc": 0.0016194911748816283,
        "lab_bench_dbqa:bpb": 0.01794504564408441,
        "lab_bench_protocolqa:bpb": 0.01638329366701465,
        "lambada:bpb": 0.005272013208484843,
        "mbpp:3shot:bpb::none": 0.007146093675157426,
        "medmcqa:bpb::none": 0.005210432526189656,
        "minerva_math_algebra:bpb::olmes": 0.002512186642526219,
        "minerva_math_counting_and_probability:bpb::olmes": 0.017516373078626597,
        "minerva_math_geometry:bpb::olmes": 4.9743515347649376e-05,
        "minerva_math_intermediate_algebra:bpb::olmes": 0.0025928043668242556,
        "minerva_math_number_theory:bpb::olmes": 0.004094694481595177,
        "minerva_math_prealgebra:bpb::olmes": 0.002023501724195347,
        "minerva_math_precalculus:bpb::olmes": 0.0170815889134783,
        "mmlu_abstract_algebra:bpb::olmes": 0.008046748596576793,
        "mmlu_anatomy:bpb::olmes": 0.009178255537913979,
        "mmlu_astronomy:bpb::olmes": 0.0066227742594237565,
        "mmlu_business_ethics:bpb::olmes": 0.004232571061243686,
        "mmlu_clinical_knowledge:bpb::olmes": 0.0033713720205329816,
        "mmlu_college_biology:bpb::olmes": 0.0022847236027105384,
        "mmlu_college_chemistry:bpb::olmes": 0.014674015550051019,
        "mmlu_college_computer_science:bpb::olmes": 0.021289663763667695,
        "mmlu_college_mathematics:bpb::olmes": 0.0035058400710138928,
        "mmlu_college_medicine:bpb::olmes": 0.0030974105629151365,
        "mmlu_college_physics:bpb::olmes": 0.009114105106536452,
        "mmlu_computer_security:bpb::olmes": 0.01790619088178488,
        "mmlu_conceptual_physics:bpb::olmes": 0.004519976713176464,
        "mmlu_econometrics:bpb::olmes": 0.000978687874355912,
        "mmlu_electrical_engineering:bpb::olmes": 0.000557685849353467,
        "mmlu_elementary_mathematics:bpb::olmes": 0.027354222303977455,
        "mmlu_formal_logic:bpb::olmes": 0.008135540574957386,
        "mmlu_global_facts:bpb::olmes": 0.001292049669275159,
        "mmlu_high_school_biology:bpb::olmes": 0.02046145805024385,
        "mmlu_high_school_chemistry:bpb::olmes": 0.011962865844193909,
        "mmlu_high_school_computer_science:bpb::olmes": 0.020430990826496433,
        "mmlu_high_school_european_history:bpb::olmes": 0.0018230741553681832,
        "mmlu_high_school_geography:bpb::olmes": 0.017297354389973377,
        "mmlu_high_school_government_and_politics:bpb::olmes": 0.00046550491882111885,
        "mmlu_high_school_macroeconomics:bpb::olmes": 0.0003151723121564308,
        "mmlu_high_school_mathematics:bpb::olmes": 0.013064719856430767,
        "mmlu_high_school_microeconomics:bpb::olmes": 0.005835973725764679,
        "mmlu_high_school_physics:bpb::olmes": 0.0001276371796803783,
        "mmlu_high_school_psychology:bpb::olmes": 0.001282850374791424,
        "mmlu_high_school_statistics:bpb::olmes": 0.002834619358631748,
        "mmlu_high_school_us_history:bpb::olmes": 0.018467976022469898,
        "mmlu_high_school_world_history:bpb::olmes": 0.0230598197638457,
        "mmlu_human_aging:bpb::olmes": 0.014267896931382298,
        "mmlu_human_sexuality:bpb::olmes": 0.0014012894995245908,
        "mmlu_international_law:bpb::olmes": 0.018301505054970506,
        "mmlu_jurisprudence:bpb::olmes": 0.006852818328723955,
        "mmlu_logical_fallacies:bpb::olmes": 0.004346208497810465,
        "mmlu_machine_learning:bpb::olmes": 0.0016162571188823927,
        "mmlu_management:bpb::olmes": 0.002026254354472209,
        "mmlu_marketing:bpb::olmes": 0.0011898541875656472,
        "mmlu_medical_genetics:bpb::olmes": 0.00743097922431955,
        "mmlu_miscellaneous:bpb::olmes": 0.01869230905954769,
        "mmlu_moral_disputes:bpb::olmes": 0.005763354936983655,
        "mmlu_moral_scenarios:bpb::olmes": 0.005651386862409582,
        "mmlu_nutrition:bpb::olmes": 0.004827279982619425,
        "mmlu_philosophy:bpb::olmes": 0.014366164601059285,
        "mmlu_prehistory:bpb::olmes": 0.01229642383266798,
        "mmlu_professional_accounting:bpb::olmes": 0.0010315645749541709,
        "mmlu_professional_law:bpb::olmes": 0.0028135445537147293,
        "mmlu_professional_medicine:bpb::olmes": 0.002090476000648137,
        "mmlu_professional_psychology:bpb::olmes": 0.012089864311680064,
        "mmlu_public_relations:bpb::olmes": 0.0007887475351766321,
        "mmlu_security_studies:bpb::olmes": 0.0015266010656280668,
        "mmlu_sociology:bpb::olmes": 0.014894776756321301,
        "mmlu_us_foreign_policy:bpb::olmes": 0.00031646285168802874,
        "mmlu_virology:bpb::olmes": 0.020847102823284926,
        "mmlu_world_religions:bpb::olmes": 0.009927598986661996,
        "mt_mbpp_v2fix:bash:bpb": 0.01764719712562093,
        "mt_mbpp_v2fix:c:bpb": 0.012974696158839685,
        "mt_mbpp_v2fix:cpp:bpb": 0.00352461874471829,
        "mt_mbpp_v2fix:csharp:bpb": 0.011264312189671633,
        "mt_mbpp_v2fix:go:bpb": 0.01499614120942969,
        "mt_mbpp_v2fix:haskell:bpb": 0.011481887350802671,
        "mt_mbpp_v2fix:java:bpb": 0.006339593337822159,
        "mt_mbpp_v2fix:javascript:bpb": 0.012447092371395432,
        "mt_mbpp_v2fix:matlab:bpb": 0.001071137141091061,
        "mt_mbpp_v2fix:php:bpb": 0.055712419903978655,
        "mt_mbpp_v2fix:python:bpb": 0.005004240008774439,
        "mt_mbpp_v2fix:r:bpb": 0.009491597701045166,
        "mt_mbpp_v2fix:ruby:bpb": 0.008193052478749744,
        "mt_mbpp_v2fix:rust:bpb": 0.0009549471189657821,
        "mt_mbpp_v2fix:scala:bpb": 0.002008104974065133,
        "mt_mbpp_v2fix:swift:bpb": 0.002333571774889047,
        "mt_mbpp_v2fix:typescript:bpb": 0.026488222629132648,
        "naturalqs:bpb::gen2mc": 0.0011019465594176115,
        "piqa:bpb::olmes:full": 0.015126604666699642,
        "socialiqa:bpb::olmes:full": 0.0027698715646968324,
        "squad:bpb::gen2mc": 0.02760320830467563,
        "winogrande:bpb::olmes:full": 8.895219435553339e-05,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-small olmo3:dev:1b:macro:bpb \
    #     --tasks-dev-target olmo3:dev:7b:macro \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:1b:macro:bpb:w_avg": {
        "codex_humaneval:3shot:bpb::none": 0.014126000163522818,
        "coqa:bpb::gen2mc": 0.032171168871277286,
        "csqa:bpb::olmes:full": 0.005933061892211778,
        "drop:bpb::gen2mc": 0.31134192128112576,
        "hellaswag:bpb::olmes:full": 0.03425290011396938,
        "jeopardy:bpb::gen2mc": 0.046497415934567195,
        "lab_bench_dbqa:bpb": 0.01986798213412343,
        "lab_bench_protocolqa:bpb": 0.03786351879635187,
        "lambada:bpb": 0.09653305992001528,
        "mbpp:3shot:bpb::none": 0.18397388768796274,
        "medmcqa:bpb::none": 0.06302713890367868,
        "mmlu:bpb": 0.00826975948170866,
        "naturalqs:bpb::gen2mc": 0.0027356055313584597,
        "piqa:bpb::olmes:full": 0.07317983995931955,
        "socialiqa:bpb::olmes:full": 0.02415271095276176,
        "squad:bpb::gen2mc": 0.03413505168414939,
        "winogrande:bpb::olmes:full": 0.011938976691896084,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-small olmo3:dev:1b:math:bpb \
    #     --tasks-dev-target olmo3:dev:7b:math \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:1b:math:bpb:w_avg": {
        "minerva_math_algebra:bpb::olmes": 0.023594290068469814,
        "minerva_math_counting_and_probability:bpb::olmes": 0.07201395027296896,
        "minerva_math_geometry:bpb::olmes": 0.08579470824530411,
        "minerva_math_intermediate_algebra:bpb::olmes": 0.010323985038546915,
        "minerva_math_number_theory:bpb::olmes": 0.004765810811492077,
        "minerva_math_prealgebra:bpb::olmes": 0.7850974366012409,
        "minerva_math_precalculus:bpb::olmes": 0.018409818961977274,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-small olmo3:dev:1b:code:bpb \
    #     --tasks-dev-target olmo3:dev:7b:code \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:1b:code:bpb:w_avg": {
        "codex_humaneval:3shot:bpb::none": 0.018626848287839844,
        "mbpp:3shot:bpb::none": 0.018591543974339528,
        "mt_mbpp_v2fix:bash:bpb": 0.20358867691368757,
        "mt_mbpp_v2fix:c:bpb": 0.04723149866741683,
        "mt_mbpp_v2fix:cpp:bpb": 0.015667698499412876,
        "mt_mbpp_v2fix:csharp:bpb": 0.01926661976893688,
        "mt_mbpp_v2fix:go:bpb": 0.003216683067931399,
        "mt_mbpp_v2fix:haskell:bpb": 0.3831671707731747,
        "mt_mbpp_v2fix:java:bpb": 0.025646885655698122,
        "mt_mbpp_v2fix:javascript:bpb": 0.04062711791944858,
        "mt_mbpp_v2fix:matlab:bpb": 0.008246527196355685,
        "mt_mbpp_v2fix:php:bpb": 0.04613274408787622,
        "mt_mbpp_v2fix:python:bpb": 0.0169537441416735,
        "mt_mbpp_v2fix:r:bpb": 0.014015909935696673,
        "mt_mbpp_v2fix:ruby:bpb": 0.016530811573389894,
        "mt_mbpp_v2fix:rust:bpb": 0.01526091556558665,
        "mt_mbpp_v2fix:scala:bpb": 0.06675987160919454,
        "mt_mbpp_v2fix:swift:bpb": 0.02369446462742231,
        "mt_mbpp_v2fix:typescript:bpb": 0.016774267734918323,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-target olmo3:dev:7b:macro \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:7b:macro:w_avg": {
        "arc:mc::full": 0.02753973414155245,
        "basic:rc": 0.025394627602338623,
        "codex_humaneval:3shot::olmo3": 0.049170080954526706,
        "coqa::olmes": 0.027535499706089452,
        "coqa:mc::gen2mc": 0.042047170928952375,
        "cruxeval_input:pass@5": 0.04355878785377479,
        "cruxeval_output:pass@5": 0.0017101356451009035,
        "csqa:mc::olmes:full": 0.03113760258094617,
        "drop::olmes": 0.03157416180561762,
        "drop:mc::gen2mc": 0.020151240964271143,
        "gsm8k::olmes": 0.06125318220100964,
        "hellaswag:rc::olmes:full": 0.005974560903766294,
        "jeopardy::olmes": 0.05757535793651844,
        "jeopardy:mc::gen2mc": 0.035713631039113666,
        "lambada": 0.03197302769115251,
        "mbpp:3shot::olmo3": 0.01465194919812231,
        "medmcqa:mc::none": 0.04025609725021448,
        "medqa_en:mc::none": 0.018282203671518626,
        "minerva": 0.014189986495333717,
        "mmlu:mc": 0.008162429418198962,
        "naturalqs::olmes": 0.0060504593611125515,
        "naturalqs:mc::gen2mc": 0.07581560572640905,
        "piqa:mc::olmes:full": 0.023959568194194508,
        "sciq:mc::olmo3": 0.014761596712930512,
        "socialiqa:mc::olmes:full": 0.05446944236338374,
        "squad::olmes": 0.01665638353619562,
        "squad:mc::gen2mc": 0.14827699836741665,
        "winogrande:rc::olmes:full": 0.0721584777502385,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-target olmo3:dev:7b:micro \
    #     --dashboard olmo-3-baseline -P
    "olmo3:dev:7b:micro:w_avg": {
        "arc_challenge:mc::olmes:full": 0.016466030885418974,
        "arc_easy:mc::olmes:full": 0.007459640950968453,
        "codex_humaneval:3shot::olmo3": 0.00196973336497905,
        "coqa::olmes": 0.007993426912611781,
        "coqa:mc::gen2mc": 0.0003867164957878487,
        "cruxeval_input:pass@5": 0.017209588732826818,
        "cruxeval_output:pass@5": 0.001066679650677994,
        "csqa:mc::olmes:full": 0.0003979850268278439,
        "drop::olmes": 0.028278490804606777,
        "drop:mc::gen2mc": 0.004320319745622549,
        "gsm8k::olmes": 0.007997649066139678,
        "gsm_symbolic::olmo3": 0.02524892107020496,
        "gsm_symbolic:p1::olmo3": 0.002365241306747053,
        "gsm_symbolic:p2::olmo3": 0.0353533807937371,
        "hellaswag:rc::olmes:full": 0.006617316524146125,
        "jeopardy::olmes": 0.00825698825538478,
        "jeopardy:mc::gen2mc": 0.005802579720448536,
        "lambada": 0.009683872213175234,
        "mbpp:3shot::olmo3": 0.015421811989074741,
        "medmcqa:mc::none": 0.010428756305961925,
        "medqa_en:mc::none": 0.021778138783424196,
        "minerva_math_algebra::olmes": 0.0024647713035461536,
        "minerva_math_counting_and_probability::olmes": 0.02660807704084201,
        "minerva_math_geometry::olmes": 0.006266955839973054,
        "minerva_math_intermediate_algebra::olmes": 0.006584608882601187,
        "minerva_math_number_theory::olmes": 0.0005271738415726158,
        "minerva_math_prealgebra::olmes": 0.0030307343540250716,
        "minerva_math_precalculus::olmes": 0.00018924702820516398,
        "mmlu:mc": 0.013946594852336331,
        "mmlu_abstract_algebra:mc::olmes": 0.01155709139469866,
        "mmlu_anatomy:mc::olmes": 0.0025206935573361174,
        "mmlu_astronomy:mc::olmes": 0.011653190628682156,
        "mmlu_business_ethics:mc::olmes": 0.007868942374955888,
        "mmlu_clinical_knowledge:mc::olmes": 0.021909686766622003,
        "mmlu_college_biology:mc::olmes": 7.930201106861664e-05,
        "mmlu_college_chemistry:mc::olmes": 0.005745878643839664,
        "mmlu_college_computer_science:mc::olmes": 0.002113641337425486,
        "mmlu_college_mathematics:mc::olmes": 0.0027751603297574427,
        "mmlu_college_medicine:mc::olmes": 0.0022566172333647466,
        "mmlu_college_physics:mc::olmes": 0.016188831070128983,
        "mmlu_computer_security:mc::olmes": 0.0025480432193317154,
        "mmlu_conceptual_physics:mc::olmes": 0.05434259819563279,
        "mmlu_econometrics:mc::olmes": 0.006816230553401793,
        "mmlu_electrical_engineering:mc::olmes": 0.04669505402564458,
        "mmlu_elementary_mathematics:mc::olmes": 0.026154163936604757,
        "mmlu_formal_logic:mc::olmes": 0.005733154555432161,
        "mmlu_global_facts:mc::olmes": 0.032067061263674125,
        "mmlu_high_school_biology:mc::olmes": 0.0004402885498605287,
        "mmlu_high_school_chemistry:mc::olmes": 0.003920523260892913,
        "mmlu_high_school_computer_science:mc::olmes": 6.396472303206553e-05,
        "mmlu_high_school_european_history:mc::olmes": 0.0021535623048203594,
        "mmlu_high_school_geography:mc::olmes": 0.0021669610045507584,
        "mmlu_high_school_government_and_politics:mc::olmes": 0.008179786129359375,
        "mmlu_high_school_macroeconomics:mc::olmes": 0.008389723574861924,
        "mmlu_high_school_mathematics:mc::olmes": 0.010148388953258031,
        "mmlu_high_school_microeconomics:mc::olmes": 0.0027658461453579112,
        "mmlu_high_school_physics:mc::olmes": 0.01078613848667904,
        "mmlu_high_school_psychology:mc::olmes": 0.013154979697719393,
        "mmlu_high_school_statistics:mc::olmes": 0.0010860457916099934,
        "mmlu_high_school_us_history:mc::olmes": 0.01255808797668154,
        "mmlu_high_school_world_history:mc::olmes": 0.010118685614294305,
        "mmlu_human_aging:mc::olmes": 0.00037090591988581047,
        "mmlu_human_sexuality:mc::olmes": 0.00923862052236967,
        "mmlu_international_law:mc::olmes": 0.0037765542518853846,
        "mmlu_jurisprudence:mc::olmes": 0.00789398583074267,
        "mmlu_logical_fallacies:mc::olmes": 0.03447396416074928,
        "mmlu_machine_learning:mc::olmes": 0.0066375880951647,
        "mmlu_management:mc::olmes": 0.003491488651447483,
        "mmlu_marketing:mc::olmes": 0.016273627246796263,
        "mmlu_medical_genetics:mc::olmes": 0.01137740782218691,
        "mmlu_miscellaneous:mc::olmes": 0.024886379834953194,
        "mmlu_moral_disputes:mc::olmes": 0.03390894204568508,
        "mmlu_moral_scenarios:mc::olmes": 0.007428238331571243,
        "mmlu_nutrition:mc::olmes": 0.0019326183380770961,
        "mmlu_philosophy:mc::olmes": 0.006085427498987143,
        "mmlu_prehistory:mc::olmes": 0.0008485048669759796,
        "mmlu_professional_accounting:mc::olmes": 0.003834733196190452,
        "mmlu_professional_law:mc::olmes": 0.014233999090516238,
        "mmlu_professional_medicine:mc::olmes": 0.014906862483433942,
        "mmlu_professional_psychology:mc::olmes": 0.04580821744999965,
        "mmlu_public_relations:mc::olmes": 0.013432203485639246,
        "mmlu_security_studies:mc::olmes": 0.008410545487616489,
        "mmlu_sociology:mc::olmes": 0.00903473081232678,
        "mmlu_us_foreign_policy:mc::olmes": 0.02219852426910933,
        "mmlu_virology:mc::olmes": 0.005539599235381527,
        "mmlu_world_religions:mc::olmes": 0.00780738559435324,
        "naturalqs::olmes": 0.011793230980533592,
        "naturalqs:mc::gen2mc": 0.0006095600628126257,
        "piqa:mc::olmes:full": 0.003966169575930312,
        "sciq:mc::olmo3": 0.004220642697566293,
        "socialiqa:mc::olmes:full": 0.006745796561332668,
        "squad::olmes": 0.004406104954963072,
        "squad:mc::gen2mc": 0.0014037231432681922,
        "winogrande:rc::olmes:full": 0.013943962449095128,
    },
    # olmo-cookbook compute-weights \
    #     --tasks-dev-target olmo3:dev:7b:math \
    #     --dashboard olmo-3-baseline -P
    # "olmo3:dev:7b:math:w_avg": {},
    # olmo-cookbook compute-weights \
    #     --tasks-dev-target olmo3:dev:7b:code \
    #     --dashboard olmo-3-baseline -P
    # "olmo3:dev:7b:code:w_avg": {},
}
