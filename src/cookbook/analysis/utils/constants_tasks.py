def get_title_from_task(task):
    if isinstance(task, list):
        title_mapping = {
            'mmlu_pro_': 'mmlu_pro',
            'mmlu_abstract_algebra:mc': 'mmlu_mc',
            'mmlu': 'mmlu',
            'minerva': 'minerva',
            'agi_eval': 'agi_eval',
            'bbh': 'bbh',
            'arc_challenge:para': 'olmes_core9_para',
            'arc_challenge:distractors': 'olmes_core9_distractors',
            'arc_challenge:enlarge': 'olmes_core9_enlarge',
            'arc_challenge:mc': 'olmes_core9_mc',
            'arc_challenge': 'olmes_core9',
            'drop': 'olmes_gen',
        }
        for key, title in title_mapping.items():
            if key in task[0]:
                return title
        return 'aggregate'
    return task


def set_title_from_task(ax, task):
    ax.set_title(get_title_from_task(task))

RC_TASKS_OLMES = [
    "arc_challenge:rc::olmes:full",
    "arc_easy:rc::olmes:full",
    "boolq:rc::olmes:full",
    "csqa:rc::olmes:full",
    "hellaswag:rc::olmes:full",
    "openbookqa:rc::olmes:full",
    "piqa:rc::olmes:full",
    "socialiqa:rc::olmes:full",
    "winogrande:rc::olmes:full",
    "mmlu_abstract_algebra:rc::olmes:full",
    "mmlu_anatomy:rc::olmes:full",
    "mmlu_astronomy:rc::olmes:full",
    "mmlu_business_ethics:rc::olmes:full",
    "mmlu_clinical_knowledge:rc::olmes:full",
    "mmlu_college_biology:rc::olmes:full",
    "mmlu_college_chemistry:rc::olmes:full",
    "mmlu_college_computer_science:rc::olmes:full",
    "mmlu_college_mathematics:rc::olmes:full",
    "mmlu_college_medicine:rc::olmes:full",
    "mmlu_college_physics:rc::olmes:full",
    "mmlu_computer_security:rc::olmes:full",
    "mmlu_conceptual_physics:rc::olmes:full",
    "mmlu_econometrics:rc::olmes:full",
    "mmlu_electrical_engineering:rc::olmes:full",
    "mmlu_elementary_mathematics:rc::olmes:full",
    "mmlu_formal_logic:rc::olmes:full",
    "mmlu_global_facts:rc::olmes:full",
    "mmlu_high_school_biology:rc::olmes:full",
    "mmlu_high_school_chemistry:rc::olmes:full",
    "mmlu_high_school_computer_science:rc::olmes:full",
    "mmlu_high_school_european_history:rc::olmes:full",
    "mmlu_high_school_geography:rc::olmes:full",
    "mmlu_high_school_government_and_politics:rc::olmes:full",
    "mmlu_high_school_macroeconomics:rc::olmes:full",
    "mmlu_high_school_mathematics:rc::olmes:full",
    "mmlu_high_school_microeconomics:rc::olmes:full",
    "mmlu_high_school_physics:rc::olmes:full",
    "mmlu_high_school_psychology:rc::olmes:full",
    "mmlu_high_school_statistics:rc::olmes:full",
    "mmlu_high_school_us_history:rc::olmes:full",
    "mmlu_high_school_world_history:rc::olmes:full",
    "mmlu_human_aging:rc::olmes:full",
    "mmlu_human_sexuality:rc::olmes:full",
    "mmlu_international_law:rc::olmes:full",
    "mmlu_jurisprudence:rc::olmes:full",
    "mmlu_logical_fallacies:rc::olmes:full",
    "mmlu_machine_learning:rc::olmes:full",
    "mmlu_management:rc::olmes:full",
    "mmlu_marketing:rc::olmes:full",
    "mmlu_medical_genetics:rc::olmes:full",
    "mmlu_miscellaneous:rc::olmes:full",
    "mmlu_moral_disputes:rc::olmes:full",
    "mmlu_moral_scenarios:rc::olmes:full",
    "mmlu_nutrition:rc::olmes:full",
    "mmlu_philosophy:rc::olmes:full",
    "mmlu_prehistory:rc::olmes:full",
    "mmlu_professional_accounting:rc::olmes:full",
    "mmlu_professional_law:rc::olmes:full",
    "mmlu_professional_medicine:rc::olmes:full",
    "mmlu_professional_psychology:rc::olmes:full",
    "mmlu_public_relations:rc::olmes:full",
    "mmlu_security_studies:rc::olmes:full",
    "mmlu_sociology:rc::olmes:full",
    "mmlu_us_foreign_policy:rc::olmes:full",
    "mmlu_virology:rc::olmes:full",
    "mmlu_world_religions:rc::olmes:full",
]

MC_TASKS_OLMES          = [task.replace(":rc", ":mc") for task in RC_TASKS_OLMES]
PARA_TASKS_OLMES        = [task.replace(":rc", ":para") for task in RC_TASKS_OLMES]
ENLARGE_TASKS_OLMES     = [task.replace(":rc", ":enlarge") for task in RC_TASKS_OLMES if 'winogrande' not in task and 'mmlu' not in task]
DISTRACTORS_TASKS_OLMES = [task.replace(":rc", ":distractors") for task in RC_TASKS_OLMES if 'winogrande' not in task and 'mmlu' not in task]

MC_TASKS_COPY_COLORS = [
    "copycolors_2way:mc::none",
    "copycolors_cyclic_2way:mc::none",
    "copycolors_4way:mc::none",
    "copycolors_cyclic_4way:mc::none",
    "copycolors_8way:mc::none",
    "copycolors_cyclic_8way:mc::none",
]

PALOMA = [
    "paloma_4chan_meta_sep::paloma",
    # "paloma_c4_100_domains::paloma", # 28K
    "paloma_c4_en::paloma",
    "paloma_dolma_100_programing_languages::paloma",
    # "paloma_dolma_100_subreddits::paloma", # 92K
    "paloma_dolma-v1_5::paloma",
    "paloma_falcon-refinedweb::paloma",
    "paloma_gab::paloma",
    "paloma_m2d2_s2orc_unsplit::paloma",
    "paloma_m2d2_wikipedia_unsplit::paloma",
    "paloma_manosphere_meta_sep::paloma",
    "paloma_mc4::paloma",
    "paloma_ptb::paloma",
    "paloma_redpajama::paloma",
    # "paloma_twitterAAE_HELM_fixed::paloma", # 100K
    "paloma_wikitext_103::paloma",
]
LLM_COMPRESSION = [
    "arxiv_math::llm_compression",
    "cc::llm_compression",
    "python::llm_compression",
]
CUSTOM_LOSS = [
    'sky_t1::custom_loss', 
    'numia_math::custom_loss', 
    'tulu_if::custom_loss'
]

GEN_TASKS_OLMES = [
    # Core generation-based benchmarks
    # "coqa::olmes:full", # <- coqa is not setup properly (no few shot examples)
    "drop::olmes:full",
    "gsm8k::olmes:full",
    "jeopardy::olmes:full",
    "naturalqs::olmes:full",
    "squad::olmes:full",
    "triviaqa::olmes:full",
]
GEN_TASKS_OLMES_PERTURB_RC = [task.replace('::olmes', ':perturb_rc::olmes') for task in GEN_TASKS_OLMES]

MMLU_PRO_MC = [
    "mmlu_pro_math:mc::none",
    "mmlu_pro_health:mc::none",
    "mmlu_pro_physics:mc::none",
    "mmlu_pro_business:mc::none",
    "mmlu_pro_biology:mc::none",
    "mmlu_pro_chemistry:mc::none",
    "mmlu_pro_computer science:mc::none",
    "mmlu_pro_economics:mc::none",
    "mmlu_pro_engineering:mc::none",
    "mmlu_pro_philosophy:mc::none",
    "mmlu_pro_other:mc::none",
    "mmlu_pro_history:mc::none",
    "mmlu_pro_psychology:mc::none",
    "mmlu_pro_law:mc::none",
]
MMLU_PRO_RC          = [task.replace(":mc::none", ":rc::none") for task in MMLU_PRO_MC]
MMLU_PRO_COT         = [task.replace(":mc::none", ":cot::none") for task in MMLU_PRO_MC]
# MMLU_PRO_COT         = [task.replace(":mc::none", ":cot::llama3.1") for task in MMLU_PRO_MC] # <- broken on base models
# MMLU_PRO_PERTURB_COT = [task.replace(":mc", ":perturb_cot") for task in MMLU_PRO_MC] # <- does not exist yet!

AGI_EVAL_MC = [
    # AGI Eval MC
    "agi_eval_lsat-ar::olmes:full",
    "agi_eval_lsat-lr::olmes:full",
    "agi_eval_lsat-rc::olmes:full",
    "agi_eval_logiqa-en::olmes:full",
    "agi_eval_sat-math::olmes:full",
    "agi_eval_sat-en::olmes:full",
    "agi_eval_aqua-rat::olmes:full",
    "agi_eval_sat-en-without-passage::olmes:full",
    "agi_eval_gaokao-english::olmes:full",
]
AGI_EVAL_RC = [task.replace("::olmes:full", ":rc::none") for task in AGI_EVAL_MC]
AGI_EVAL_COT = [task.replace("::olmes:full", ":cot::none") for task in AGI_EVAL_MC] # ::tulu3 does not work on base models. only this config works currently

MINERVA_MC = [
    # Minerva does not have MC
]

MINERVA_COT = [
    # Minerva CoT (there's also a tulu and llama config)
    "minerva_math_algebra::olmes:full",
    "minerva_math_counting_and_probability::olmes:full",
    "minerva_math_geometry::olmes:full",
    "minerva_math_intermediate_algebra::olmes:full",
    "minerva_math_number_theory::olmes:full",
    "minerva_math_prealgebra::olmes:full",
    "minerva_math_precalculus::olmes:full",
]

BBH_COT = [
    # BBH COT (generation-based)
    "bbh_boolean_expressions:cot::olmes:full",
    "bbh_causal_judgement:cot::olmes:full",
    "bbh_date_understanding:cot::olmes:full",
    "bbh_disambiguation_qa:cot::olmes:full",
    "bbh_dyck_languages:cot::olmes:full",
    "bbh_formal_fallacies:cot::olmes:full",
    "bbh_geometric_shapes:cot::olmes:full",
    "bbh_hyperbaton:cot::olmes:full",
    "bbh_logical_deduction_five_objects:cot::olmes:full",
    "bbh_logical_deduction_seven_objects:cot::olmes:full",
    "bbh_logical_deduction_three_objects:cot::olmes:full",
    "bbh_movie_recommendation:cot::olmes:full",
    "bbh_multistep_arithmetic_two:cot::olmes:full",
    "bbh_navigate:cot::olmes:full",
    "bbh_object_counting:cot::olmes:full",
    "bbh_penguins_in_a_table:cot::olmes:full",
    "bbh_reasoning_about_colored_objects:cot::olmes:full",
    "bbh_ruin_names:cot::olmes:full",
    "bbh_salient_translation_error_detection:cot::olmes:full",
    "bbh_snarks:cot::olmes:full",
    "bbh_sports_understanding:cot::olmes:full",
    "bbh_temporal_sequences:cot::olmes:full",
    "bbh_tracking_shuffled_objects_five_objects:cot::olmes:full",
    "bbh_tracking_shuffled_objects_seven_objects:cot::olmes:full",
    "bbh_tracking_shuffled_objects_three_objects:cot::olmes:full",
    "bbh_web_of_lies:cot::olmes:full",
    "bbh_word_sorting:cot::olmes:full",
]

BBH_MC = [
    # QA Variants of BBH Tasks
    "bbh_boolean_expressions:qa::none",
    "bbh_causal_judgement:qa::none",
    "bbh_date_understanding:qa::none",
    "bbh_disambiguation_qa:qa::none",
    "bbh_dyck_languages:qa::none",
    "bbh_formal_fallacies:qa::none",
    "bbh_geometric_shapes:qa::none",
    "bbh_hyperbaton:qa::none",
    "bbh_logical_deduction_five_objects:qa::none",
    "bbh_logical_deduction_seven_objects:qa::none",
    "bbh_logical_deduction_three_objects:qa::none",
    "bbh_movie_recommendation:qa::none",
    "bbh_multistep_arithmetic_two:qa::none",
    "bbh_navigate:qa::none",
    "bbh_object_counting:qa::none",
    "bbh_penguins_in_a_table:qa::none",
    "bbh_reasoning_about_colored_objects:qa::none",
    "bbh_ruin_names:qa::none",
    "bbh_salient_translation_error_detection:qa::none",
    "bbh_snarks:qa::none",
    "bbh_sports_understanding:qa::none",
    "bbh_temporal_sequences:qa::none",
    "bbh_tracking_shuffled_objects_five_objects:qa::none",
    "bbh_tracking_shuffled_objects_seven_objects:qa::none",
    "bbh_tracking_shuffled_objects_three_objects:qa::none",
    "bbh_web_of_lies:qa::none",
    "bbh_word_sorting:qa::none",
]

PERTURB_COT_TASKS = [
    'agi_eval_aqua-rat:perturb_cot::olmes', 
    'agi_eval_gaokao-english:perturb_cot::olmes', 
    'agi_eval_logiqa-en:perturb_cot::olmes', 
    'agi_eval_lsat-ar:perturb_cot::olmes', 
    'agi_eval_lsat-lr:perturb_cot::olmes', 
    'agi_eval_lsat-rc:perturb_cot::olmes', 
    'agi_eval_sat-en-without-passage:perturb_cot::olmes', 
    'agi_eval_sat-en:perturb_cot::olmes', 
    'agi_eval_sat-math:perturb_cot::olmes', 
    
    'bbh_boolean_expressions:perturb_cot::olmes', 
    'bbh_causal_judgement:perturb_cot::olmes', 
    'bbh_date_understanding:perturb_cot::olmes', 
    'bbh_disambiguation_qa:perturb_cot::olmes', 
    'bbh_dyck_languages:perturb_cot::olmes', 
    'bbh_formal_fallacies:perturb_cot::olmes', 
    'bbh_geometric_shapes:perturb_cot::olmes', 
    'bbh_hyperbaton:perturb_cot::olmes', 
    'bbh_logical_deduction_five_objects:perturb_cot::olmes', 
    'bbh_logical_deduction_seven_objects:perturb_cot::olmes', 
    'bbh_logical_deduction_three_objects:perturb_cot::olmes', 
    'bbh_movie_recommendation:perturb_cot::olmes', 
    'bbh_multistep_arithmetic_two:perturb_cot::olmes', 
    'bbh_navigate:perturb_cot::olmes', 
    'bbh_object_counting:perturb_cot::olmes', 
    'bbh_penguins_in_a_table:perturb_cot::olmes', 
    'bbh_reasoning_about_colored_objects:perturb_cot::olmes', 
    'bbh_ruin_names:perturb_cot::olmes', 
    'bbh_salient_translation_error_detection:perturb_cot::olmes', 
    'bbh_snarks:perturb_cot::olmes', 
    'bbh_sports_understanding:perturb_cot::olmes', 
    'bbh_temporal_sequences:perturb_cot::olmes', 
    'bbh_tracking_shuffled_objects_five_objects:perturb_cot::olmes', 
    'bbh_tracking_shuffled_objects_seven_objects:perturb_cot::olmes', 
    'bbh_tracking_shuffled_objects_three_objects:perturb_cot::olmes', 
    'bbh_web_of_lies:perturb_cot::olmes', 
    'bbh_word_sorting:perturb_cot::olmes', 
    
    'gsm8k:perturb_cot::olmes', 
    
    'minerva_math_algebra:perturb_cot::olmes', 
    'minerva_math_counting_and_probability:perturb_cot::olmes', 
    'minerva_math_geometry:perturb_cot::olmes', 
    'minerva_math_intermediate_algebra:perturb_cot::olmes', 
    'minerva_math_number_theory:perturb_cot::olmes', 
    'minerva_math_prealgebra:perturb_cot::olmes', 
    # TODO: Add the final minerva tasks
]

ADDITIONAL_TASKS_TULU_3 = [
    # (Additional Tulu tasks I've excluded for now)

    # CoT (exact match)
    "tydiqa_english::tulu",
    "ifeval::tulu",
    "truthfulqa::tulu",
    "alpaca_eval_v2::tulu",

    # CoT Long Context (exact match)
    "zero_scrolls_gov_report::tulu",
    "zero_scrolls_summ_screen_fd::tulu",
    "zero_scrolls_qmsum::tulu",
    "zero_scrolls_qasper::tulu",
    "zero_scrolls_narrative_qa::tulu",
    "zero_scrolls_quality::tulu",

    # Code benchmarks
    "bigcodebench::tulu",
    "bigcodebench_hard::tulu",
    "codex_humaneval:temp0.1",
    "codex_humaneval:temp0.8",
    "codex_humaneval::tulu",
    "codex_humanevalplus::tulu"
]

AGI_EVAL_TULU_3 = [
    # AGI Eval CoT (only ::tulu3 has proper configs) -- broken
    "agi_eval_lsat-ar:0shot_cot::tulu3",
    "agi_eval_lsat-lr:0shot_cot::tulu3",
    "agi_eval_lsat-rc:0shot_cot::tulu3",
    "agi_eval_logiqa-en:0shot_cot::tulu3",
    "agi_eval_sat-math:0shot_cot::tulu3",
    "agi_eval_sat-en:0shot_cot::tulu3",
    "agi_eval_aqua-rat:0shot_cot::tulu3",
    "agi_eval_sat-en-without-passage:0shot_cot::tulu3",
    "agi_eval_gaokao-english:0shot_cot::tulu3",
]

PRIMARY_METRICS_OLMES = {
    "aime": "exact_match_flex",
    "alpaca_eval": "win_rate",
    "arc_challenge": "acc_uncond",
    "arc_challenge:mc": "acc_raw",
    "arc_easy": "acc_per_char",
    "arc_easy:mc": "acc_raw",
    "autobencher": "logits_per_byte",
    "autobencher:mc": "acc_raw",
    "autobencher_math": "exact_match",
    "bigcodebench": "pass_at_1",
    "bigcodebench_hard": "pass_at_1",
    "boolq": "acc_raw",
    "boolq:mc": None,
    "custom_loss_sky_t1": "bits_per_byte",
    "custom_loss_numia_math": "bits_per_byte",
    "custom_loss_tulu_if": "bits_per_byte",
    "codex_humaneval": "pass_at_1",
    "codex_humanevalplus": "pass_at_1",
    "copa": "acc_raw",
    "copycolors": "acc_uncond",
    "copycolors:mc": "acc_raw",
    "coqa": "f1",
    "cosmosqa": "acc_per_char",
    "cosmosqa:mc": "acc_raw",
    "csqa": "acc_uncond",
    "csqa:mc": "acc_raw",
    "drop": "f1",
    "gsm8k": "exact_match",
    "gsm8k_selfc": "maj_at_10",
    "gsm_plus": "exact_match",
    "gsm_plus_selfc": None,
    "gsm_symbolic_main": "exact_match",
    "gsm_symbolic_p1": "exact_match",
    "gsm_symbolic_p2": "exact_match",
    "gpqa": "exact_match",
    "hellaswag": "acc_per_char",
    "hellaswag:mc": "acc_raw",
    "ifeval": "inst_level_loose_acc",
    "jeopardy": "f1",
    "logiqa": "acc_per_char",
    "logiqa:mc": "acc_raw",
    "minerva_math_algebra": None,
    "minerva_math_counting_and_probability": None,
    "minerva_math_geometry": None,
    "minerva_math_intermediate_algebra": None,
    "minerva_math_number_theory": None,
    "minerva_math_prealgebra": None,
    "minerva_math_precalculus": None,
    "minerva_math_500": "exact_match",
    "mbpp": "pass_at_1",
    "mbppplus": "pass_at_1",
    "medmcqa": "acc_per_char",
    "medmcqa:mc": "acc_per_char",
    "mmlu_abstract_algebra:mc": None,
    "mmlu_abstract_algebra": None,
    "mmlu_abstract_algebra:cot": "exact_match",
    "mmlu_anatomy:mc": None,
    "mmlu_anatomy": None,
    "mmlu_anatomy:cot": "exact_match",
    "mmlu_astronomy:mc": None,
    "mmlu_astronomy": None,
    "mmlu_astronomy:cot": "exact_match",
    "mmlu_business_ethics:mc": None,
    "mmlu_business_ethics": None,
    "mmlu_business_ethics:cot": "exact_match",
    "mmlu_clinical_knowledge:mc": None,
    "mmlu_clinical_knowledge": None,
    "mmlu_clinical_knowledge:cot": "exact_match",
    "mmlu_college_biology:mc": None,
    "mmlu_college_biology": None,
    "mmlu_college_biology:cot": "exact_match",
    "mmlu_college_chemistry:mc": None,
    "mmlu_college_chemistry": None,
    "mmlu_college_chemistry:cot": "exact_match",
    "mmlu_college_computer_science:mc": None,
    "mmlu_college_computer_science": None,
    "mmlu_college_computer_science:cot": "exact_match",
    "mmlu_college_mathematics:mc": None,
    "mmlu_college_mathematics": None,
    "mmlu_college_mathematics:cot": "exact_match",
    "mmlu_college_medicine:mc": None,
    "mmlu_college_medicine": None,
    "mmlu_college_medicine:cot": "exact_match",
    "mmlu_college_physics:mc": None,
    "mmlu_college_physics": None,
    "mmlu_college_physics:cot": "exact_match",
    "mmlu_computer_security:mc": None,
    "mmlu_computer_security": None,
    "mmlu_computer_security:cot": "exact_match",
    "mmlu_conceptual_physics:mc": None,
    "mmlu_conceptual_physics": None,
    "mmlu_conceptual_physics:cot": "exact_match",
    "mmlu_econometrics:mc": None,
    "mmlu_econometrics": None,
    "mmlu_econometrics:cot": "exact_match",
    "mmlu_electrical_engineering:mc": None,
    "mmlu_electrical_engineering": None,
    "mmlu_electrical_engineering:cot": "exact_match",
    "mmlu_elementary_mathematics:mc": None,
    "mmlu_elementary_mathematics": None,
    "mmlu_elementary_mathematics:cot": "exact_match",
    "mmlu_formal_logic:mc": None,
    "mmlu_formal_logic": None,
    "mmlu_formal_logic:cot": "exact_match",
    "mmlu_global_facts:mc": None,
    "mmlu_global_facts": None,
    "mmlu_global_facts:cot": "exact_match",
    "mmlu_high_school_biology:mc": None,
    "mmlu_high_school_biology": None,
    "mmlu_high_school_biology:cot": "exact_match",
    "mmlu_high_school_chemistry:mc": None,
    "mmlu_high_school_chemistry": None,
    "mmlu_high_school_chemistry:cot": "exact_match",
    "mmlu_high_school_computer_science:mc": None,
    "mmlu_high_school_computer_science": None,
    "mmlu_high_school_computer_science:cot": "exact_match",
    "mmlu_high_school_european_history:mc": None,
    "mmlu_high_school_european_history": None,
    "mmlu_high_school_european_history:cot": "exact_match",
    "mmlu_high_school_geography:mc": None,
    "mmlu_high_school_geography": None,
    "mmlu_high_school_geography:cot": "exact_match",
    "mmlu_high_school_government_and_politics:mc": None,
    "mmlu_high_school_government_and_politics": None,
    "mmlu_high_school_government_and_politics:cot": "exact_match",
    "mmlu_high_school_macroeconomics:mc": None,
    "mmlu_high_school_macroeconomics": None,
    "mmlu_high_school_macroeconomics:cot": "exact_match",
    "mmlu_high_school_mathematics:mc": None,
    "mmlu_high_school_mathematics": None,
    "mmlu_high_school_mathematics:cot": "exact_match",
    "mmlu_high_school_microeconomics:mc": None,
    "mmlu_high_school_microeconomics": None,
    "mmlu_high_school_microeconomics:cot": "exact_match",
    "mmlu_high_school_physics:mc": None,
    "mmlu_high_school_physics": None,
    "mmlu_high_school_physics:cot": "exact_match",
    "mmlu_high_school_psychology:mc": None,
    "mmlu_high_school_psychology": None,
    "mmlu_high_school_psychology:cot": "exact_match",
    "mmlu_high_school_statistics:mc": None,
    "mmlu_high_school_statistics": None,
    "mmlu_high_school_statistics:cot": "exact_match",
    "mmlu_high_school_us_history:mc": None,
    "mmlu_high_school_us_history": None,
    "mmlu_high_school_us_history:cot": "exact_match",
    "mmlu_high_school_world_history:mc": None,
    "mmlu_high_school_world_history": None,
    "mmlu_high_school_world_history:cot": "exact_match",
    "mmlu_human_aging:mc": None,
    "mmlu_human_aging": None,
    "mmlu_human_aging:cot": "exact_match",
    "mmlu_human_sexuality:mc": None,
    "mmlu_human_sexuality": None,
    "mmlu_human_sexuality:cot": "exact_match",
    "mmlu_international_law:mc": None,
    "mmlu_international_law": None,
    "mmlu_international_law:cot": "exact_match",
    "mmlu_jurisprudence:mc": None,
    "mmlu_jurisprudence": None,
    "mmlu_jurisprudence:cot": "exact_match",
    "mmlu_logical_fallacies:mc": None,
    "mmlu_logical_fallacies": None,
    "mmlu_logical_fallacies:cot": "exact_match",
    "mmlu_machine_learning:mc": None,
    "mmlu_machine_learning": None,
    "mmlu_machine_learning:cot": "exact_match",
    "mmlu_management:mc": None,
    "mmlu_management": None,
    "mmlu_management:cot": "exact_match",
    "mmlu_marketing:mc": None,
    "mmlu_marketing": None,
    "mmlu_marketing:cot": "exact_match",
    "mmlu_medical_genetics:mc": None,
    "mmlu_medical_genetics": None,
    "mmlu_medical_genetics:cot": "exact_match",
    "mmlu_miscellaneous:mc": None,
    "mmlu_miscellaneous": None,
    "mmlu_miscellaneous:cot": "exact_match",
    "mmlu_moral_disputes:mc": None,
    "mmlu_moral_disputes": None,
    "mmlu_moral_disputes:cot": "exact_match",
    "mmlu_moral_scenarios:mc": None,
    "mmlu_moral_scenarios": None,
    "mmlu_moral_scenarios:cot": "exact_match",
    "mmlu_nutrition:mc": None,
    "mmlu_nutrition": None,
    "mmlu_nutrition:cot": "exact_match",
    "mmlu_philosophy:mc": None,
    "mmlu_philosophy": None,
    "mmlu_philosophy:cot": "exact_match",
    "mmlu_prehistory:mc": None,
    "mmlu_prehistory": None,
    "mmlu_prehistory:cot": "exact_match",
    "mmlu_professional_accounting:mc": None,
    "mmlu_professional_accounting": None,
    "mmlu_professional_accounting:cot": "exact_match",
    "mmlu_professional_law:mc": None,
    "mmlu_professional_law": None,
    "mmlu_professional_law:cot": "exact_match",
    "mmlu_professional_medicine:mc": None,
    "mmlu_professional_medicine": None,
    "mmlu_professional_medicine:cot": "exact_match",
    "mmlu_professional_psychology:mc": None,
    "mmlu_professional_psychology": None,
    "mmlu_professional_psychology:cot": "exact_match",
    "mmlu_public_relations:mc": None,
    "mmlu_public_relations": None,
    "mmlu_public_relations:cot": "exact_match",
    "mmlu_security_studies:mc": None,
    "mmlu_security_studies": None,
    "mmlu_security_studies:cot": "exact_match",
    "mmlu_sociology:mc": None,
    "mmlu_sociology": None,
    "mmlu_sociology:cot": "exact_match",
    "mmlu_us_foreign_policy:mc": None,
    "mmlu_us_foreign_policy": None,
    "mmlu_us_foreign_policy:cot": "exact_match",
    "mmlu_virology:mc": None,
    "mmlu_virology": None,
    "mmlu_virology:cot": "exact_match",
    "mmlu_world_religions:mc": None,
    "mmlu_world_religions": None,
    "mmlu_world_religions:cot": "exact_match",
    "mmlu_pro_math:cot": "exact_match",
    "mmlu_pro_health:cot": "exact_match",
    "mmlu_pro_physics:cot": "exact_match",
    "mmlu_pro_business:cot": "exact_match",
    "mmlu_pro_biology:cot": "exact_match",
    "mmlu_pro_chemistry:cot": "exact_match",
    "mmlu_pro_computer science:cot": "exact_match",
    "mmlu_pro_economics:cot": "exact_match",
    "mmlu_pro_engineering:cot": "exact_match",
    "mmlu_pro_philosophy:cot": "exact_match",
    "mmlu_pro_other:cot": "exact_match",
    "mmlu_pro_history:cot": "exact_match",
    "mmlu_pro_psychology:cot": "exact_match",
    "mmlu_pro_law:cot": "exact_match",
    "mmlu_pro_math": None,
    "mmlu_pro_health": None,
    "mmlu_pro_physics": None,
    "mmlu_pro_business": None,
    "mmlu_pro_biology": None,
    "mmlu_pro_chemistry": None,
    "mmlu_pro_computer science": None,
    "mmlu_pro_economics": None,
    "mmlu_pro_engineering": None,
    "mmlu_pro_philosophy": None,
    "mmlu_pro_other": None,
    "mmlu_pro_history": None,
    "mmlu_pro_psychology": None,
    "mmlu_pro_law": None,
    "mmlu_pro_math:rc": None,
    "mmlu_pro_health:rc": None,
    "mmlu_pro_physics:rc": None,
    "mmlu_pro_business:rc": None,
    "mmlu_pro_biology:rc": None,
    "mmlu_pro_chemistry:rc": None,
    "mmlu_pro_computer science:rc": None,
    "mmlu_pro_economics:rc": None,
    "mmlu_pro_engineering:rc": None,
    "mmlu_pro_philosophy:rc": None,
    "mmlu_pro_other:rc": None,
    "mmlu_pro_history:rc": None,
    "mmlu_pro_psychology:rc": None,
    "mmlu_pro_law:rc": None,
    "mt_eval_refinement_single": "llm_score",
    "mt_eval_refinement_multi": "llm_score",
    "mt_eval_expansion_single": "llm_score",
    "mt_eval_expansion_multi": "llm_score",
    "mt_eval_follow-up_single": "llm_score",
    "mt_eval_follow-up_multi": "llm_score",
    "mt_eval_recollection_single_cls": "llm_score",
    "mt_eval_recollection_multi_cls": "llm_score",
    "mt_eval_recollection_single_global-inst": "llm_score",
    "mt_eval_recollection_multi_global-inst": "llm_score",
    "naturalqs_open": "f1",
    "openbookqa": "acc_uncond",
    "openbookqa:mc": "acc_raw",
    "paloma_4chan_meta_sep": None,
    "paloma_c4_100_domains": None,
    "paloma_c4_en": None,
    "paloma_dolma_100_programing_languages": None,
    "paloma_dolma_100_subreddits": None,
    "paloma_dolma-v1_5": None,
    "paloma_falcon-refinedweb": None,
    "paloma_gab": None,
    "paloma_m2d2_s2orc_unsplit": None,
    "paloma_m2d2_wikipedia_unsplit": None,
    "paloma_manosphere_meta_sep": None,
    "paloma_mc4": None,
    "paloma_ptb": None,
    "paloma_redpajama": None,
    "paloma_twitterAAE_HELM_fixed": None,
    "paloma_wikitext_103": None,
    "llm_compression_arxiv_math": None,
    "llm_compression_cc": None,
    "llm_compression_python": None,
    "piqa": "acc_per_char",
    "piqa:mc": "acc_raw",
    "popqa": "exact_match",
    "sciq": "acc_raw",
    "socialiqa": "acc_per_char",
    "socialiqa:mc": "acc_raw",
    "squad": "f1",
    "squad2": "f1",
    "triviaqa": "f1",
    "truthfulqa": "mc1",
    "tydiqa_english": None,
    "tydiqa_arabic": None,
    "tydiqa_bengali": None,
    "tydiqa_finnish": None,
    "tydiqa_indonesian": None,
    "tydiqa_korean": None,
    "tydiqa_russian": None,
    "tydiqa_swahili": None,
    "tydiqa_telugu": None,
    "winogrande": "acc_raw",
    "winogrande:mc": "acc_raw",
    "zero_scrolls_gov_report": "rougeL_f1",
    "zero_scrolls_summ_screen_fd": "rougeL_f1",
    "zero_scrolls_qmsum": "rougeL_f1",
    "zero_scrolls_qasper": "f1",
    "zero_scrolls_narrative_qa": "f1",
    "zero_scrolls_quality": "exact_match",
    "arc_challenge:para": "acc_per_char",
    "arc_easy:para": "acc_per_char",
    "boolq:para": "acc_raw",
    "csqa:para": "acc_uncond",
    "hellaswag:para": "acc_per_char",
    "openbookqa:para": "acc_uncond",
    "piqa:para": "acc_per_char",
    "socialiqa:para": "acc_per_char",
    "winogrande:para": "acc_raw",
    "mmlu_abstract_algebra:para": "acc_per_char",
    "mmlu_anatomy:para": "acc_per_char",
    "mmlu_astronomy:para": "acc_per_char",
    "mmlu_business_ethics:para": "acc_per_char",
    "mmlu_clinical_knowledge:para": "acc_per_char",
    "mmlu_college_biology:para": "acc_per_char",
    "mmlu_college_chemistry:para": "acc_per_char",
    "mmlu_college_computer_science:para": "acc_per_char",
    "mmlu_college_mathematics:para": "acc_per_char",
    "mmlu_college_medicine:para": "acc_per_char",
    "mmlu_college_physics:para": "acc_per_char",
    "mmlu_computer_security:para": "acc_per_char",
    "mmlu_conceptual_physics:para": "acc_per_char",
    "mmlu_econometrics:para": "acc_per_char",
    "mmlu_electrical_engineering:para": "acc_per_char",
    "mmlu_elementary_mathematics:para": "acc_per_char",
    "mmlu_formal_logic:para": "acc_per_char",
    "mmlu_global_facts:para": "acc_per_char",
    "mmlu_high_school_biology:para": "acc_per_char",
    "mmlu_high_school_chemistry:para": "acc_per_char",
    "mmlu_high_school_computer_science:para": "acc_per_char",
    "mmlu_high_school_european_history:para": "acc_per_char",
    "mmlu_high_school_geography:para": "acc_per_char",
    "mmlu_high_school_government_and_politics:para": "acc_per_char",
    "mmlu_high_school_macroeconomics:para": "acc_per_char",
    "mmlu_high_school_mathematics:para": "acc_per_char",
    "mmlu_high_school_microeconomics:para": "acc_per_char",
    "mmlu_high_school_physics:para": "acc_per_char",
    "mmlu_high_school_psychology:para": "acc_per_char",
    "mmlu_high_school_statistics:para": "acc_per_char",
    "mmlu_high_school_us_history:para": "acc_per_char",
    "mmlu_high_school_world_history:para": "acc_per_char",
    "mmlu_human_aging:para": "acc_per_char",
    "mmlu_human_sexuality:para": "acc_per_char",
    "mmlu_international_law:para": "acc_per_char",
    "mmlu_jurisprudence:para": "acc_per_char",
    "mmlu_logical_fallacies:para": "acc_per_char",
    "mmlu_machine_learning:para": "acc_per_char",
    "mmlu_management:para": "acc_per_char",
    "mmlu_marketing:para": "acc_per_char",
    "mmlu_medical_genetics:para": "acc_per_char",
    "mmlu_miscellaneous:para": "acc_per_char",
    "mmlu_moral_disputes:para": "acc_per_char",
    "mmlu_moral_scenarios:para": "acc_per_char",
    "mmlu_nutrition:para": "acc_per_char",
    "mmlu_philosophy:para": "acc_per_char",
    "mmlu_prehistory:para": "acc_per_char",
    "mmlu_professional_accounting:para": "acc_per_char",
    "mmlu_professional_law:para": "acc_per_char",
    "mmlu_professional_medicine:para": "acc_per_char",
    "mmlu_professional_psychology:para": "acc_per_char",
    "mmlu_public_relations:para": "acc_per_char",
    "mmlu_security_studies:para": "acc_per_char",
    "mmlu_sociology:para": "acc_per_char",
    "mmlu_us_foreign_policy:para": "acc_per_char",
    "mmlu_virology:para": "acc_per_char",
    "mmlu_world_religions:para": "acc_per_char",
    "minerva_math_geometry:perturb_cot": "acc_per_char",
    "gsm8k:perturb_cot": "acc_per_char",
    "minerva_math_intermediate_algebra:perturb_cot": "acc_per_char",
    "minerva_math_number_theory:perturb_cot": "acc_per_char",
    "minerva_math_algebra:perturb_cot": "acc_per_char",
    "minerva_math_prealgebra:perturb_cot": "acc_per_char",
    "minerva_math_counting_and_probability:perturb_cot": "acc_per_char",
    "arc_challenge:enlarge": "acc_uncond",
    "arc_easy:enlarge": "acc_per_char",
    "boolq:enlarge": "acc_raw",
    "csqa:enlarge": "acc_uncond",
    "hellaswag:enlarge": "acc_per_char",
    "openbookqa:enlarge": "acc_uncond",
    "piqa:enlarge": "acc_per_char",
    "socialiqa:enlarge": "acc_per_char",
    "arc_challenge:distractors": "acc_uncond",
    "arc_easy:distractors": "acc_per_char",
    "boolq:distractors": "acc_raw",
    "csqa:distractors": "acc_uncond",
    "hellaswag:distractors": "acc_per_char",
    "openbookqa:distractors": "acc_uncond",
    "piqa:distractors": "acc_per_char",
    "socialiqa:distractors": "acc_per_char",
    "drop:perturb_rc": "acc_per_char",
    "gsm8k:perturb_rc": "acc_per_char",
    "jeopardy:perturb_rc": "acc_per_char",
    "naturalqs:perturb_rc": "acc_per_char",
    "squad:perturb_rc": "acc_per_char",
    "triviaqa:perturb_rc": "acc_per_char",
}
