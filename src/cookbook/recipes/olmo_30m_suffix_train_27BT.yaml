name: "suffix-train-27BT-olmo2-5xC-30m-dense-falcon"
description: "suffix train with 27B tokens on 30M params on falcon sample"
budget: "ai2/oe-base"
workspace: "ai2/dolma2"
nodes: 1
gpus: 8
preemptible: true
max_tokens: 26_953_646_080
global_batch_size: 1_310_720
rank_microbatch_size: 32_768
sequence_length: 2048
seed: 1337
learning_rate: 0.007276622186288963
model: "olmo_30m"
tokenizer: "dolma2"
weka: false
priority: normal
cluster: ai2/augusta-google-1
dataset:
  sources:
    - name: falcon-refinedweb
      target_ratio: 1.0
      paths:
        - s3://ai2-llm/preprocessed/falcon-refinedweb/v0-0.05-heldout-complement-3B-sample-suffixes/allenai/dolma2-tokenizer/*.npy
  