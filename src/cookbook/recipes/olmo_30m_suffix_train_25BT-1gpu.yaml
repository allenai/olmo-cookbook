name: "suffix-train-25BT-olmo2-5xC-30m-dense-falcon-1gpu"
description: "suffix train with 25B tokens on 30M params on falcon sample"
budget: "ai2/oe-base"
workspace: "ai2/dolma2"
nodes: 1
gpus: 1
preemptible: true
max_tokens: 25_000_000_000
global_batch_size: 1214464
sequence_length: 2048
seed: 1337
learning_rate: 0.007276622186288963
model: "olmo_30m"
tokenizer: "dolma2"
weka: false
priority: normal
cluster: ai2/augusta-google-1
dataset:
  sources:
    - name: falcon-refinedweb
      target_ratio: 1.0
      paths:
        - s3://ai2-llm/preprocessed/falcon-refinedweb/v0-0.05-heldout-complement-3B-sample-suffixes/allenai/dolma2-tokenizer/*.npy
  