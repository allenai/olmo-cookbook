name: "olmo-cookbook-core-v2-1bv2-5xC-olmo2-mix-natural"
description: "Olmo-cookbook 1Bv2-5xC on olmo-2-mix, natural distribution"
budget: "ai2/oe-data"
workspace: "ai2/dolma2"
nodes: 4
gpus: 8
preemptible: true
max_tokens: 127_939_584_000 # for olmo2_1B_v2, we have  1_279_395_840 non-embedding params
sequence_length: 4096 
seed: 1337
model: "olmo2_1B_v2"
tokenizer: "dolma2"
priority: high
cluster: ai2/jupiter-cirrascale-2
weka: true
rank_microbatch_size: 32_768
global_batch_size: 2_097_152 # 4_194_304, 2_097_152
dataset:
  sources:
    - name: proofpile-2-stack
      paths:
        - s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated/algebraic-stack/train/allenai/dolma2-tokenizer/*.npy
      target_ratio: 0.0030502843358416586
    - name: proofpile-2-arxiv
      paths:
        - s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated/arxiv/train/allenai/dolma2-tokenizer/*.npy
      target_ratio: 0053613993678482625
    - name: proofpile-2-open-web-math
      paths:
        - s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated/open-web-math/train/allenai/dolma2-tokenizer/*.npy
      target_ratio: 0031448462446296357
    - name: pes2o
      paths:
        - s3://ai2-llm/preprocessed/pes2o/allenai/dolma2-tokenizer/*.npy
      target_ratio: 0.015111458687507727
    - name: starcoder
      paths:
        - s3://ai2-llm/preprocessed/starcoder/v1-decon-100_to_20k-2star-top_token_030/allenai/dolma2-tokenizer/*.npy
      target_ratio: 0.02143185698434755
    - name: dclm
      paths:
        - s3://ai2-llm/preprocessed/dclm/text_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train/allenai/dolma2-tokenizer/*.npy
      target_ratio: 0.9509550395810318
    - name: wikipedia
      paths:
        - s3://ai2-llm/preprocessed/olmo-mix/danyh-compiled-v1_7/documents/wiki/allenai/dolma2-tokenizer/*.npy
      target_ratio: 0.0009451147987934355
downstream_evaluators:
- olmo2_dev_1b
lm_evaluator: true