name: olmo2
description: OLMo 2. https://arxiv.org/abs/2501.00656
budget: ai2/oe-base
workspace: ai2/oe-data
nodes: 2
gpus: 8
preemptible: true
max_tokens: 127939584000
sequence_length: 4096
seed: 1337
model: olmo2_1B_v2
tokenizer: dolma2
priority: urgent
cluster: ai2/augusta-google-1
weka: false
rank_microbatch_size: 32768
global_batch_size: 2097152
dataset:
  sources:
  - name: olmo2
    paths:
    # 700B_olmo2_sample/
    # OLMoE_mix_0824/
    - s3://ai2-llm/preprocessed/700B_olmo2_sample/raw/allenai/dolma2-tokenizer/*.npy
    target_ratio: 1
downstream_evaluators:
- olmo2_dev_1b
lm_evaluator: true