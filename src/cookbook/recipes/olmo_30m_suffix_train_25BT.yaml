name: "suffix-train-25BT-olmo2-5xC-30m-dense-falcon"
description: "suffix train with 25B tokens on 30M params on falcon sample"
budget: "ai2/oe-base"
workspace: "ai2/dolma2"
nodes: 1
gpus: 8
preemptible: true
max_tokens: 2.5e10
global_batch_size: 1214464
sequence_length: 2048
seed: 1337
learning_rate: 0.007276622186288963
model: "olmo_30m"
tokenizer: "dolma2"
weka: false
priority: normal
cluster: ai2/augusta-google-1
dataset:
  sources:
    - name: falcon-refinedweb
      target_ratio: 1.0
      paths:
        - gs://ai2-llm/preprocessed/falcon-refinedweb/v0-0.05-heldout-complement-3B-sample-suffixes/documents/allenai/dolma2-tokenizer/*.npy
  