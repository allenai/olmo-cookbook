name: 1b-5xC-dclm-search-6T
description: 1B 5xC DCLM-only proposed mix using search (rep=4 @ for 6T, drop metrics) (162699b7dcb0e905)
budget: ai2/oe-base
workspace: ai2/dolma2
nodes: 4
gpus: 8
preemptible: true
max_tokens: 127939584000
sequence_length: 4096
seed: 1337
model: olmo2_1B_v2
tokenizer: dolma2
priority: high
cluster: ai2/jupiter
weka: true
rank_microbatch_size: 32768
global_batch_size: 2097152
dataset:
  sources:
  - name: games
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/games/dolma2-tokenizer/*.npy
    target_ratio: 0.1
    repetition_factor: 4
  - name: health
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/health/dolma2-tokenizer/*.npy
    target_ratio: 0.1
    repetition_factor: 4
  - name: literature
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/literature/dolma2-tokenizer/*.npy
    target_ratio: 0.2
    repetition_factor: 4
  - name: politics
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/politics/dolma2-tokenizer/*.npy
    target_ratio: 0.30000000000000004
    repetition_factor: 4
  - name: religion
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/religion/dolma2-tokenizer/*.npy
    target_ratio: 0.15000000000000002
    repetition_factor: 4
  - name: science_math_and_technology
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/science_math_and_technology/dolma2-tokenizer/*.npy
    target_ratio: 0.15000000000000002
    repetition_factor: 4