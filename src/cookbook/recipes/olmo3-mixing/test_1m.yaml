name: test_1m
description: 5xC 1B DCLM proposed mix (rep=4) with d=6 domains, n=7 samples, s=0 seed
budget: ai2/oe-base
workspace: ai2/dolma2
nodes: 1
gpus: 1
preemptible: true
max_tokens: 124_359_600
sequence_length: 2048
seed: 1337
model: olmo2_1M
tokenizer: dolma2
priority: high
cluster: ai2/jupiter
weka: false
rank_microbatch_size: 2048
global_batch_size: 2048
dataset:
  sources:
  - name: entertainment
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/entertainment/dolma2-tokenizer/*.npy
    target_ratio: 0.000867592666835428
    repetition_factor: 4
  - name: finance_and_business
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/finance_and_business/dolma2-tokenizer/*.npy
    target_ratio: 0.00394001348091259
    repetition_factor: 4
  - name: games
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/games/dolma2-tokenizer/*.npy
    target_ratio: 0.0004934523800221765
    repetition_factor: 4
  - name: health
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/health/dolma2-tokenizer/*.npy
    target_ratio: 0.4414242329833072
    repetition_factor: 4
  - name: politics
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/politics/dolma2-tokenizer/*.npy
    target_ratio: 0.0005621550383012255
    repetition_factor: 4
  - name: science_math_and_technology
    paths:
    - s3://ai2-llm/preprocessed/dclm/baseline_topic_classified_sample/science_math_and_technology/dolma2-tokenizer/*.npy
    target_ratio: 0.5527125534506214
    repetition_factor: 4
