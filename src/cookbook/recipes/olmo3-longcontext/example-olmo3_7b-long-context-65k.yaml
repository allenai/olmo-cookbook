name: "olmo3-7b-long-context-extension-example"
description: "OLMo3 7b swafix high-throughput configuration example"
budget: "ai2/oe-base"
workspace: "ai2/oe-data"
nodes: 2
gpus: 8
preemptible: true
max_tokens: 10_000_000_000  # 10B tokens TODO: update if needed
global_batch_size: 4194304
sequence_length: 65536
max_target_sequence_length: 65536
rank_microbatch_size: 65536  # must be the same as sequence_length
generate_doc_lengths: true  # enable intra-document masking
seed: 4123
model: "olmo3_7B_swafix"
tokenizer: "dolma2"
priority: urgent
eval_interval: 500
cluster: ai2/augusta-google-1
scheduler_type: linear
warmup_steps: 0
dp_shard_degree: 2
cp_degree: 8
float8: true
# TODO: update to the actual start checkpoint for Long Context extension
load_path: gs://ai2-llm/checkpoints/OLMo3-7B/step297000/model_and_optim/
load_state: false
downstream_evaluators:
  - olmo2_dev_1b
dataset:
  # TODO: configure target dataset sources
  sources:
  - name: dclm
    target_ratio: 0.5
    paths:
    - gs://ai2-llm/preprocessed/dclm/v0_rep32_ft7percentile_fw2/documents/allenai/dolma2-tokenizer/**/*.npy
  - name: code
    target_ratio: 0.4
    paths:
    - gs://ai2-llm/preprocessed/stack-edu/allenai/dolma2-tokenizer/**/*.npy
  - name: reasoning
    target_ratio: 0.1
    paths:
    - gs://ai2-llm/preprocessed/big-reasoning-traces/allenai/dolma2-tokenizer/*.npy
