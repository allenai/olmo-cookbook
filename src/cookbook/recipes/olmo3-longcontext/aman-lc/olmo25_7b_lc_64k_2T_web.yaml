name: &name "olmo25_7b_lc_64k_12T-midtrain_round3_qwenlike_s2pdf_gzip2080_attnsinks_10B"
description: *name
budget: "ai2/oe-base"
workspace: "ai2/long-contexts"
cluster: ai2/augusta-google-1
preemptible: true
priority: urgent
nodes: 4
gpus: 8
seed: 4123

gc_interval: 200

# annealing:
#   enabled: true

# because the learning rate went to zero during midtraining, we need to warmup to a higher learning rate
# (from end of pretraining)
warmup_steps: 200
learning_rate: 0.00020712352850360292

metrics_config:
  backend: wandb
  project: "olmo3-7b-long-context"
  entity: "ai2-llm"

generate_doc_lengths: false  # enable intra-document masking
load_path: gs://ai2-llm/checkpoints/OLMo25-from476838/step500680
load_state: false
model: "olmo25_7b"
tokenizer: "dolma2"
max_tokens: 10_000_000_000  # 10B tokens TODO: update if needed
global_batch_size: 4_194_304
sequence_length: &seq_len 65536
max_target_sequence_length: *seq_len
rank_microbatch_size: *seq_len  # must be the same as sequence_length
scheduler_type: linear

dp_shard_degree: 4  # shard_degree=1 === DDP, shard_degree=world_size === FSDP
tp_degree: 8
float8: false  # NOTE: it might be preferable to disable float8 for anneal / long context training
eval_interval: 500
# downstream_evaluators:
#   - olmo2_dev_1b
model_overrides:
  - block.attention.rope.theta=8000000
  - block.attention.use_flash=false
  - block.attention.use_flex=true
  - block.attention.use_sinks=true

activation_checkpointing: true

dataset:
  sources:
  - name: web 
    paths:
    - s3://ai2-llm/preprocessed/cc_all_dressed/all_dressed_v3_subsamples/midtrain_pools/6B/allenai/dolma2-tokenizer/*.npy