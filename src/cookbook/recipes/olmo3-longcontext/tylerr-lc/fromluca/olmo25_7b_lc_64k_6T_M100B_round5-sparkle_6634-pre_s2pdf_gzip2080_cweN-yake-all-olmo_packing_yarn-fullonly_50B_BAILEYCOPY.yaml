name: &name "olmo25_7b_lc_64k_6T_M100B_50B_BAILEY_RERUN"
description: *name
budget: ai2/oe-base
workspace: ai2/olmo-3-microanneals
cluster: ai2/triton
preemptible: true
priority: urgent
nodes: 1
gpus: 1
seed: 4123
gc_interval: 200
warmup_steps: 200
learning_rate: 0.00020712352850360292
metrics_config:
  backend: wandb
  project: bailey-testing
  entity: ai2-llm
generate_doc_lengths: true
load_state: false
model: olmo25_7b_yarn_fullonly
tokenizer: dolma2
max_tokens: 50000000000  # 50B
global_batch_size: 4194304
sequence_length: 65536
max_target_sequence_length: 65536
rank_microbatch_size: 65536
scheduler_type: linear
dp_shard_degree: 1
cp_degree: 1 # only 1 gpu
cp_head_stride: 1 # only 1 gpu
float8: true
eval_interval: 500
train_module_overrides:
- scheduler.alpha_f=0.0


dataset:
  sources:
  - name: All files
    target_ratio: 1.0
    paths:
    -  s3://ai2-llm/preprocessed/tylerr/lc-reshard-final/v0.6/allenai/dolma2-tokenizer/*.npy
