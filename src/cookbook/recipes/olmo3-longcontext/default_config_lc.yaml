# Example recipe for OLMo3 7b using the right hyperparams and context length, but missing data specification
name: "olmo3_7b_lc_64k_[data]"
description: "OLMo3 7B 64k extension using [datamix]"
budget: "ai2/oe-base"
workspace: "ai2/long-contexts"
cluster: ai2/augusta-google-1
preemptible: true
priority: urgent
nodes: 4
gpus: 8
seed: 4123

gc_interval: 200

annealing:
  enabled: true

metrics_config:
  backend: wandb
  project: "olmo3-7b-long-context"
  entity: "ai2-llm"

generate_doc_lengths: true  # enable intra-document masking
load_path: gs://ai2-llm/checkpoints/OLMo3-7B-swafix/[step]
load_state: false # no optimizer state
model: "olmo3_7B_swafix"
tokenizer: "dolma2"
max_tokens: 10_000_000_000  # 10B tokens 
global_batch_size: 4_194_304  # 4M tokens 
sequence_length: &seq_len 65536
max_target_sequence_length: *seq_len
rank_microbatch_size: *seq_len  # must be the same as sequence_length
scheduler_type: linear
warmup_steps: 200 # warmup runs for ~840M tokens
dp_shard_degree: 1  # shard_degree=1 === DDP, shard_degree=world_size === FSDP
cp_degree: 8
float8: true  # NOTE: some runs may disable for comparison to non-olmo extensions 
model_overrides:
  - block.attention.rope.theta=8000000 # extension from 500k -> 8M RoPE theta
dataset:
  sources:
  - name:
    target_ratio: 1.0 
    paths:
    - 