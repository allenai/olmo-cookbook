name: "suffix-train-olmo2-5xC-30m-dolma2-full-baseline"
description: "suffix train baseline 30M @ 5xC scale on dolma2 full dataset"
budget: "ai2/oe-base"
workspace: "ai2/dolma2"
nodes: 1
gpus: 1
preemptible: true
max_tokens: 2_910_233_600
sequence_length: 2048
global_batch_size: 131072
seed: 1337
learning_rate: 0.007276622186288963
model: "olmo_30m"
tokenizer: "dolma2"
weka: false
priority: low
cluster: ai2/titan
dataset:
  sources:
    - name: dolma2-full
      target_ratio: 1.0
      paths:
        - s3://ai2-llm/preprocessed/dolma2-0625/v0.1/allenai/dolma2-tokenizer/all-dressed-snazzy2-fixed/**/*.npy
        - s3://ai2-llm/preprocessed/dolma2-0625/v0.1/allenai/dolma2-tokenizer/arxiv/**/*.npy
        - s3://ai2-llm/preprocessed/dolma2-0625/v0.1/allenai/dolma2-tokenizer/finemath-3plus/**/*.npy
        - s3://ai2-llm/preprocessed/dolma2-0625/v0.1/allenai/dolma2-tokenizer/s2pdf_redacted/**/*.npy
        - s3://ai2-llm/preprocessed/dolma2-0625/v0.1/allenai/dolma2-tokenizer/stack-edu/**/*.npy
        - s3://ai2-llm/preprocessed/dolma2-0625/v0.1/allenai/dolma2-tokenizer/wikipedia/**/*.npy
  